{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MNIST 99 Percent\n",
    "> Aiming for 99% accuracy across the full MNIST data set.\n",
    "\n",
    "- toc: false \n",
    "- badges: true\n",
    "- comments: true\n",
    "- author: Paul D\n",
    "- image: images/2020-05-05-mnist-99-title.png\n",
    "- categories: [deep learning, fastai, gradient descent, sgd, mnist]\n",
    "- hide: false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "In my [previous blog post](https://pdito.github.io/blog/deep%20learning/fastai/gradient%20descent/sgd/mnist/2020/04/17/fastai2-ch4-mnist-first-principles.html) I ran through classification for a subset of the MNIST data (3s and 7s only) as a learning experience, following along with Fastbook chapter 4.\n",
    "\n",
    "From here, I look to take what I've learned previously to build a model to tackle the full MNIST data set, attempting to eventually hit an accuracy of > 99% on my validation set.\n",
    "\n",
    "I won't be going into as much detail for each step, so please review the previous blog post for a verbose explanation of what is going on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting Started\n",
    "\n",
    "As usual, we start by importing the necessary libraries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastai2.vision.all import *\n",
    "from utils import *\n",
    "import itertools\n",
    "import math\n",
    "\n",
    "matplotlib.rc('image', cmap='Greys')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use fastai's built in ```untar_data``` function to download and extract the full MNIST data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(#2) [Path('/home/pdito/.fastai/data/mnist_png/training'),Path('/home/pdito/.fastai/data/mnist_png/testing')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = untar_data(URLs.MNIST)\n",
    "path.ls()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like we have both training and testing data sub-folders. In this case, we'll use the testing data as our validation data.\n",
    "\n",
    "*Note: Really we should split our training data into training and validation data and keep our testing data separate, but since we are not building something that will ever make it into production, we use the simpler approach.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we iterate through all the images to create a list of all our training and validation images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = (path/'training').ls().sorted()\n",
    "valid_path = (path/'testing').ls().sorted()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_images = list(itertools.chain.from_iterable(([x.ls().sorted() for x in train_path])))\n",
    "valid_images = list(itertools.chain.from_iterable(([x.ls().sorted() for x in valid_path])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then convert the list into a tensor, where dimension 0 represents each individual image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = [tensor(Image.open(o)) for o in train_images]\n",
    "valid_x = [tensor(Image.open(o)) for o in valid_images]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000, 28, 28]), torch.Size([10000, 28, 28]))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = torch.stack(train_x).float()/255\n",
    "valid_x = torch.stack(valid_x).float()/255\n",
    "train_x.shape, valid_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, the tensor is then reshaped to combine the row and column pixel images into one long tensor, row by row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([60000, 784])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_x = train_x.view(-1, 28*28)\n",
    "valid_x = valid_x.view(-1, 28*28)\n",
    "train_x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to create our labels. We can use the same list we used to create our ```train_x``` and ```valid_x``` tensors, iterating though to generate a tensor of values (in this case an int for the number) based on the parent folder name of the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([60000]), torch.Size([10000]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = torch.stack([tensor(int(os.path.basename(os.path.dirname(o)))) for o in train_images])\n",
    "valid_y = torch.stack([tensor(int(os.path.basename(os.path.dirname(o)))) for o in valid_images])\n",
    "train_y.shape, valid_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the zip function to create a list of tuples for the images and their labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dset = list(zip(train_x, train_y))\n",
    "valid_dset = list(zip(valid_x, valid_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have our training set we can create our ```Dataloaders```, which pass mini-batches of our data to our training model. Note, its typically good practice to shuffle our training data. In our example, this step is **essential**. Since if we don't shuffle, most mini-batches will contain images of only one number (as our data set is ordered by folder)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_dset, batch_size=256, shuffle=True)\n",
    "valid_dl = DataLoader(valid_dset, batch_size=256, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create a function which can be used to randomly initialise our parameters, applying ```.requires_grad_()```to tell PyTorch to calculate our gradients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create our model. In this case we are starting with a simple linear model, wx + b. We are however applying a log softmax function to the result. Softmax in effect squashes our output vector to values between 0 and 1, where those values sum to 1. Our output vector can be interpreted as the probability of something belonging to a given class.\n",
    "\n",
    "We also take the log of the results, for reasons which are explained next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(xb): return torch.log_softmax((xb@weights + bias), 1)\n",
    "\n",
    "#Example of log_softmax\n",
    "#def log_softmax(x):\n",
    "#    return x - x.exp().sum(-1).log().unsqueeze(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our loss function, we use Negative Log Likelihood. The better our prediction, the lower the NLL. This function focuses only on our prediction for what would have been the correct class. \n",
    "\n",
    "As an example, lets assume our dataset only contains numbers 1 - 4. For a given image, our softmax output is ```[0.1, 0.1, 0.1, 0.7]``` and our label tensor is ```[0, 0, 0, 1]```. In this case our NLL is the negative log of ```(0 * 0.1) + (0 * 0.1) + (0 * 0.1) + (1 * 0.7)```. In other words ```-ln(0.7) = 0.155```.\n",
    "\n",
    "In that case, our model was making a correct guess with 70% confidence. Let's now look at the example where that guess was incorrect, by changing our label tensor to ```[0, 0, 1, 0]```. In this case our NLL is the negative log of ```(0 * 0.1) + (0 * 0.1) + (1 * 0.1) + (0 * 0.7) = -ln(0.1) = 1```. So a much higher loss. \n",
    "\n",
    "The reason we took the log of softmax earlier is because the ```nll_loss``` function expects its input to be the log of probabilities as opposed to the probabilities themselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mnist_loss(predictions, targets):\n",
    "    return F.nll_loss(predictions, targets)\n",
    "\n",
    "loss_func = mnist_loss\n",
    "\n",
    "#Example of NLL\n",
    "#def mnist_loss_manual(predictions, targets): return -predictions[range(targets.shape[0]), targets].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we define our step process, which calculates our predictions for a given mini-batch, calculates the loss of those predictions using our loss function and then calculates the gradients of our parameters based on that loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_grad(xb, yb, model):\n",
    "    preds = model(xb)\n",
    "    loss = loss_func(preds, yb)\n",
    "    loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then create our training function, which represents an entire epoch. In this case it loops through every mini_batch, calculating the gradients, adjusting our parameters by their gradient multiplied by the learning rate and then resetting the gradients to zero (since they are additive otherwise, which is not what we want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, lr, params):\n",
    "    for xb, yb in train_dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        for p in params:\n",
    "            p.data -= p.grad * lr\n",
    "            p.grad.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Right now we only have a loss to measure performance. This is great for training, but not great for us to know how we're doing. Below we create a function that outputs the accuracy of a given mini batch (taking the index of our highest probability prediction and comparing that to our label for each image).\n",
    "\n",
    "We then create a function that performs this on our entire validation set that we can call after each training epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_accuracy(xb, yb):\n",
    "    preds = torch.argmax(xb, dim=1)\n",
    "    return (preds == yb).float().mean()\n",
    "\n",
    "def validate_epoch(model):\n",
    "    accs = [batch_accuracy(model(xb), yb) for xb, yb in valid_dl]\n",
    "    return round(torch.stack(accs).mean().item(), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it's time to set our model in to action. We start by initialising our parameters, and we also define a learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = init_params((28*28,10))\n",
    "bias = init_params(10)\n",
    "params = weights, bias\n",
    "lr = 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Press play..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.853223 0.86416 0.875488 0.880273 0.892285 0.890625 0.894043 0.891699 0.897949 0.900977 0.899805 0.905762 0.906055 0.901367 0.908789 0.906543 0.90957 0.904102 0.900391 0.906934 "
     ]
    }
   ],
   "source": [
    " def train_model(model, epochs, lr):\n",
    "    for i in range(epochs):\n",
    "        train_epoch(model, lr, params)\n",
    "        print(validate_epoch(model), end=' ')\n",
    "\n",
    "train_model(model, 20, lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 90% accuracy. Pretty good for a simple linear model. Our model struggles to improve much beyond the 10th epch, perhaps a learning rate that is too high."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Up Code with PyTorch/fastai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's simplify our code by using PyTorch's built in nn.Linear to create our model. This also handles parameter initialisation for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_model = nn.Linear(28*28, 10)\n",
    "w, b = linear_model.parameters()\n",
    "lr = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are no longer taking the log_softmax in our model, we can introduce the PyTorch loss function F.cross_entropy which combines both log softmax and NLL into one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = F.cross_entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To tidy thing up, we can also wrap our step and zero grad functions into an optimiser class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicOptim:\n",
    "    def __init__(self, params, lr): self.params, self.lr = list(params), lr\n",
    "    \n",
    "    def step(self, *args, **kwargs):\n",
    "        for p in self.params: p.data -= p.grad.data * self.lr\n",
    "    \n",
    "    def zero_grad(self, *args, **kwargs):\n",
    "        for p in self.params: p.grad = None\n",
    "            \n",
    "opt = BasicOptim(linear_model.parameters(), lr)\n",
    "\n",
    "def train_epoch_lm(model):\n",
    "    for xb, yb in train_dl:\n",
    "        calc_grad(xb, yb, model)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "        \n",
    "def train_model_lm(model, epochs):\n",
    "    for i in range(epochs):\n",
    "        train_epoch_lm(model)\n",
    "        print(validate_epoch(model), end=' ')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.88291 0.895898 0.902441 0.902734 0.909668 0.907422 0.911035 0.90957 0.911621 0.912793 0.912793 0.915137 0.91416 0.914746 0.916406 0.915918 0.918066 0.915918 0.916406 0.918164 "
     ]
    }
   ],
   "source": [
    "train_model_lm(linear_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Better results than before, around 92% accuracy. Why, since nothing changed in our actual model architecture?\n",
    "\n",
    "Actually, something did change, we reduced the learning rate from 1.0 to 0.1. Everything else remain consistent, just represented in a cleaner way using less and more reusable code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing BasicOptim with SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To further simplify, fastai provides us with a built in SGD class, similar to the BasicOptim class we created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.883691 0.894336 0.901367 0.903223 0.908203 0.908984 0.910156 0.911523 0.91543 0.912305 0.916895 0.916309 0.914453 0.915723 0.91582 0.918555 0.916309 0.915332 0.917773 0.918164 "
     ]
    }
   ],
   "source": [
    "linear_model = nn.Linear(28*28, 10)\n",
    "opt = SGD(linear_model.parameters(), lr)\n",
    "train_model_lm(linear_model, 20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, 92% accuracy. Similar results, which makes sense, since nothing has changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using fastai Learner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, before we start to improve our model, we use a fastai ```Learner```to replace out training loop in order to further simplify our code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.521860</td>\n",
       "      <td>0.446785</td>\n",
       "      <td>0.889100</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.408741</td>\n",
       "      <td>0.378355</td>\n",
       "      <td>0.899300</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.373681</td>\n",
       "      <td>0.350100</td>\n",
       "      <td>0.906000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.355128</td>\n",
       "      <td>0.335873</td>\n",
       "      <td>0.907800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.347222</td>\n",
       "      <td>0.323164</td>\n",
       "      <td>0.912600</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.331390</td>\n",
       "      <td>0.315915</td>\n",
       "      <td>0.914400</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.320100</td>\n",
       "      <td>0.309472</td>\n",
       "      <td>0.915100</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.315955</td>\n",
       "      <td>0.305583</td>\n",
       "      <td>0.915400</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.307917</td>\n",
       "      <td>0.301444</td>\n",
       "      <td>0.916400</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.318447</td>\n",
       "      <td>0.298375</td>\n",
       "      <td>0.916900</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.304005</td>\n",
       "      <td>0.296194</td>\n",
       "      <td>0.917900</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.305967</td>\n",
       "      <td>0.292694</td>\n",
       "      <td>0.918400</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.305479</td>\n",
       "      <td>0.290779</td>\n",
       "      <td>0.918000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.300242</td>\n",
       "      <td>0.289566</td>\n",
       "      <td>0.920500</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.304179</td>\n",
       "      <td>0.287598</td>\n",
       "      <td>0.919700</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.301372</td>\n",
       "      <td>0.286153</td>\n",
       "      <td>0.920600</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.295946</td>\n",
       "      <td>0.285350</td>\n",
       "      <td>0.920200</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.294676</td>\n",
       "      <td>0.285005</td>\n",
       "      <td>0.920200</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.293234</td>\n",
       "      <td>0.284260</td>\n",
       "      <td>0.920200</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.292301</td>\n",
       "      <td>0.282514</td>\n",
       "      <td>0.919800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dls = DataLoaders(train_dl, valid_dl)\n",
    "learn = Learner(dls, nn.Linear(28*28,10), opt_func=SGD, loss_func=F.cross_entropy, metrics=batch_accuracy)\n",
    "learn.fit(20, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "92% accuracy here too, just what we wanted to see since again, nothing has changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Non-Linearity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we do want things to change. To improve our model, let's add some non-linearity. We'll sandwich a ReLU activation function in between two linear layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_net = nn.Sequential(\n",
    "    nn.Linear(28*28, 30),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(30,10)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>batch_accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.480248</td>\n",
       "      <td>0.397933</td>\n",
       "      <td>0.890400</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.339421</td>\n",
       "      <td>0.317796</td>\n",
       "      <td>0.910300</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.307536</td>\n",
       "      <td>0.290654</td>\n",
       "      <td>0.917800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.281915</td>\n",
       "      <td>0.269593</td>\n",
       "      <td>0.922500</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.263428</td>\n",
       "      <td>0.251829</td>\n",
       "      <td>0.927800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.247231</td>\n",
       "      <td>0.242583</td>\n",
       "      <td>0.930700</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.223604</td>\n",
       "      <td>0.224113</td>\n",
       "      <td>0.935700</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.221571</td>\n",
       "      <td>0.218439</td>\n",
       "      <td>0.936400</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.209643</td>\n",
       "      <td>0.217177</td>\n",
       "      <td>0.936700</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.196226</td>\n",
       "      <td>0.199739</td>\n",
       "      <td>0.942700</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.194212</td>\n",
       "      <td>0.195394</td>\n",
       "      <td>0.943300</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>0.189708</td>\n",
       "      <td>0.186436</td>\n",
       "      <td>0.947200</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>0.183434</td>\n",
       "      <td>0.182419</td>\n",
       "      <td>0.947000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>0.175635</td>\n",
       "      <td>0.173868</td>\n",
       "      <td>0.949500</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>0.166307</td>\n",
       "      <td>0.172731</td>\n",
       "      <td>0.949800</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>0.159394</td>\n",
       "      <td>0.169198</td>\n",
       "      <td>0.950900</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>0.160116</td>\n",
       "      <td>0.163204</td>\n",
       "      <td>0.952200</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>0.151481</td>\n",
       "      <td>0.160232</td>\n",
       "      <td>0.952000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>0.154878</td>\n",
       "      <td>0.161813</td>\n",
       "      <td>0.953000</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>0.146765</td>\n",
       "      <td>0.154272</td>\n",
       "      <td>0.955700</td>\n",
       "      <td>00:01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = Learner(dls, neural_net, opt_func=SGD, loss_func=F.cross_entropy, metrics=batch_accuracy)\n",
    "learn.fit(20, 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A pretty significant improvement, over 95% accurate, still using just a very simple architecture. Looking at the output we could definitely afford to train this over more epochs and expect continued improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using ResNet18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we want to achieve an accuracy of over 99%, let's use a more complex neural net, in this case, the infamous ResNet18 architecture.\n",
    "\n",
    "We want to use fastai's convenience methods for this, so we use the```DataBlock``` function to ensure our data is presented in the desired format.\n",
    "\n",
    "We have two blocks, an ```ImageBlock``` (our data) and a ```CategoryBlock``` (our labels). We use ```PILImage``` even though our images are greyscale (which would be ```PILImageBW```) as ResNet18 was designed to be used on RGB images and expects its inputs to be structured accordingly.\n",
    "\n",
    "```get_image_files``` is a helper function that returns all the images under the path.\n",
    "\n",
    "```GrandparenterSplitter``` let's us specify the training and validation data split by the images' parent's parent (ie. grandparent) folder.\n",
    "\n",
    "```parent_label```let's us define our image labels as the folder name they are contained within.\n",
    "\n",
    "We then run ```dataloaders```on our ```DataBlock```to get our ```DataLoaders```.\n",
    "\n",
    "*Note: nothing actually runs in the DataBlock until we call its dataloaders property against a path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = DataBlock(blocks=(ImageBlock(cls=PILImage), CategoryBlock), \n",
    "                  get_items=get_image_files, \n",
    "                  splitter=GrandparentSplitter(train_name='training', valid_name='testing'),\n",
    "                  get_y=parent_label)\n",
    "\n",
    "dls = mnist.dataloaders(untar_data(URLs.MNIST), batch_sz=128)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create our Learner, using the resnet18 architecture without pretrained weights. We also directly reference ```F.cross_entropy```in our Learner and use the fastai's built in ```accuracy``` metric. We use fastai's ```.fit_one_cycle``` training method which is a more sophisticated version of ```.fit```.\n",
    "\n",
    "I'm sure we'll blog about this soon, but you can read more [here](https://docs.fast.ai/callbacks.one_cycle.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.189028</td>\n",
       "      <td>0.167352</td>\n",
       "      <td>0.946300</td>\n",
       "      <td>00:27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.099857</td>\n",
       "      <td>0.090754</td>\n",
       "      <td>0.975600</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.052010</td>\n",
       "      <td>0.045006</td>\n",
       "      <td>0.987300</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.021180</td>\n",
       "      <td>0.017461</td>\n",
       "      <td>0.994700</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.013487</td>\n",
       "      <td>0.015505</td>\n",
       "      <td>0.994900</td>\n",
       "      <td>00:28</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, success. 99.5% accuracy after just 5 epochs and two and half minutes of training. We achieve this result in just 4 lines of code. A good indication of the power of fastai."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Doing Something Ridiculous Like Using ResNet152"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as an aside, let's try an extremely deep model to see if we get any improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0.551158</td>\n",
       "      <td>0.449306</td>\n",
       "      <td>0.858300</td>\n",
       "      <td>02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.264751</td>\n",
       "      <td>0.383378</td>\n",
       "      <td>0.880300</td>\n",
       "      <td>02:03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.075151</td>\n",
       "      <td>0.046404</td>\n",
       "      <td>0.985700</td>\n",
       "      <td>01:58</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.063741</td>\n",
       "      <td>0.031806</td>\n",
       "      <td>0.990700</td>\n",
       "      <td>02:05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.015324</td>\n",
       "      <td>0.021084</td>\n",
       "      <td>0.994100</td>\n",
       "      <td>02:03</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "learn = cnn_learner(dls, resnet152, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy)\n",
    "learn.fit_one_cycle(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, no additional benefit from further complexity.\n",
    "\n",
    "Of course, we could try more epochs, but this comes at risk of overfitting. Investigating what our model got wrong and using that to form the basis of our next steps would be the best way forward. But for now, we're content with our >99%.\n",
    "\n",
    "Thanks for reading."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "split_at_heading": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
