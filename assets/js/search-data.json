{
  
    
        "post0": {
            "title": "A Pokemon Gen 1 Classifier",
            "content": "The Project . In my initial blog post I talked about why I decided to pursue deep learning more actively, and how I intended to reach my goals, one small step at a time. . So, on that note following completion of week 2 of the fastai 2020 course, here is my first project, a Pokemon Gen 1 Classifier. My target is to achieve 90% accuracy metric. As I write, I have no idea how ambitious that is! . The Data . One thing i&#39;ve learned very quickly about Deep Learning, is that gathering and labelling data is often the biggest challenge. Thankfully, i&#39;ve also learned that the community is incredibly generous in sharing data. I definitely intend to gather my own data soon - I feel that&#39;s something key to get experience in early on, but for my first project I wanted to dive into the coding as soon as possible. . I found an excellent dataset on Kaggle perfectly suited to my problem. So big thanks to Lance Zhang for this. . Steps . I&#39;m trying to keep this model as simple as possible, and hope the below steps will cover everything. . Set up. | Get data and build data model. | Train model. | Test on new data. | Productionise. | 1. Set Up . I&#39;m fortunate to have a local Ubuntu box with a GPU capable of training these kind of models (I&#39;ll post about setting that up soon). But typically it is recommended to use something like Paperspace or Colab when starting out and the below should be applicable to any environment you intend to run Jupyter on. . First step, import the necessary libraries. . from utils import * from fastai2.vision.widgets import * . TIP: This initially failed for me. I had to ensure that utils.py, which sits in the fast.ai nbs (notebooks) directory, was copied to the same folder as the notebook I was working on. . 2. The Data Model . There are many ways to get data into your deep learning environment. Kaggle offers a library, which I&#39;ll attempt to use in future, but for now, what I found most comfortable was to download the data to my local machine, inspect it, get to know it, zip it up and put that zip file on Dropbox. I was then able to share that file, and use wget {link} to import it to my deep learning environment. . One point worth noting, is that when you share a Dropbox link, the end part always defaults to dl=0, you should change this to dl=1 for the wget command to work. I believe this option is only available to paid Dropbox accounts, so please bear that in mind too. . After this step I manually unzipped the file. fast.ai has some nice convenience methods to download and untar data, but for now I don&#39;t believe the library has native support for zipped data. . Below I set the path for my data. . path = Path(&quot;/home/pdito/AI/datasets/pokemon-gen1/&quot;) path.ls() . (#150) [Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Psyduck&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Bulbasaur&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Poliwhirl&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Raticate&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Victreebel&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Tangela&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Venusaur&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Butterfree&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Cubone&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Gastly&#39;)...] . We can see the data is split into folders for each different Pokemon, but there is no training / validation split currently. . Next I create my Datablock. . I have two blocks, and ImageBlock and a CategoryBlock. The ImageBlock represents by independent variables, whilst the CategoryBlock represents the dependent value, in other words, my y or what i&#39;m trying to predict. . In order to get the items, I set this to get_image_files - which recursively polls a provided path to extract all the images. I&#39;m actually not sure why we don&#39;t specify the path in the DataBlock, and instead call it later when we initialise the DataLoader. I guess the DataBlock is simply defining the structure and the DataLoader is actually initialising something with real data. . Splitter determines my training / validation set split, i&#39;ve chosen to make this 80% / 20% and set a seed so the validation set remains consistent as I iterate. . I set my Category / dependent variable / y to parent_label, as my images are all in subfolders with the Pokemon name. This is super useful part of the fastai library, and I understand there are lots of different options available. . Finally, I transform all my images using a standard Resize. This makes the images 224x224, which should work well as i&#39;m intending to use transfer learning from ResNet34, which was trained on images of this dimension. . pokemon = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(224)) . Next, from my DataBlock, I create my DataLoaders (train and valid) referencing the root folder of my images that we already specified above. . dls = pokemon.dataloaders(path) . I can now visualise the DataLoaders to make sure everything looks correct. . dls.valid.show_batch(max_n=4, nrows=1) . All looks good, so its time to define our architecture. In this case . learn = cnn_learner(dls, resnet34, metrics=accuracy) learn.fine_tune(4) . epoch train_loss valid_loss accuracy time . 0 | 4.545189 | 1.752553 | 0.603968 | 00:13 | . epoch train_loss valid_loss accuracy time . 0 | 1.387421 | 0.610193 | 0.854519 | 00:18 | . 1 | 0.632813 | 0.358847 | 0.911830 | 00:18 | . 2 | 0.244674 | 0.301103 | 0.929464 | 00:18 | . 3 | 0.120268 | 0.288353 | 0.930198 | 00:18 | . Surprisingly, with almost no effort, I&#39;m able to achieve over 93% accuracy on the validation set. This is particularly impressive given the structure of the input data. Images include the official animation images, pixelated gameboy images, plush toy images, hand-drawn images. Furthermore, many of the Pokemon in gen 1 have an extremely close likeness amongst their evolution tree. . I had expected to have to fine tune the model and tidy up the input data to get anywhere near this result. Instead, in less than a minute and half of training, i&#39;ve smashed my initial target. . This is a great indication of the power of transfer learning. I&#39;m keen to revisit this model in future to see how I can improve the accuracy. . Even though I am happy with the results, it&#39;s interesting to look at the items that the model is most incorrect on (in other words, from the classifications it got wrong, those which it has most confidence it got correct). . Often these results are visualised in a confusion matrix, but we have too many classes here (151) so instead we just straight to the top losses. . interp = ClassificationInterpretation.from_learner(learn) . #interp.plot_top_losses(5, nrows=1) interp.plot_top_losses(6, figsize=(15,10)) . Seeing this visualisation helps me understand why the model is making mistakes. . Top Left: This image should be excluded. It shows multiple Pokemon. Top Middle: So close! It was obviously hard for the model to pick up detail in this dimly lit image. Top Right: Mislabelled data! Our model is actually correct. This is Alakazam and not Kadabra. Bottom Left: I feel like our model should have got this one, but admitedly the angle is strange. Bottm Middle: It&#39;s unusual to see Marowak with fire, and the colour palette is far more attuned to Magmar. Bottom Right: You can certainly see why the model would choose Gyarados. At a glance, I would do the same. . 4. Quick Test . I use a widget to easily allow me to upload files for testing from my local machine. Here I run a test against an image of Pikachu that is not included in the original data set. . uploader = widgets.FileUpload() uploader . img = PILImage.create(uploader.data[0]) category,pred_idx,probs = learn.predict(img) print(f&quot;Who is this?: {category}. Probability: {probs[pred_idx] * 100:.02f}%.&quot;) img . Who is this?: Pikachu. Probability: 99.96%. . It works! . 5. Productionise . If getting good, labelled data is the hardest part of the problem, i&#39;m told productionising your model comes a close second. . I&#39;m going to build this out into a separate notebook and use Voila and Binder to publish it on the web. Nethertheless, for completeness I&#39;ll provide all the detail below . First, let&#39;s export our trained model and confirm that the file exists after doing so. . learn.export() path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . I&#39;m renaming the file in terminal from export.pkl to pokemon.pkl for my reference. . Everything below this point will form the basis of our &#39;Web App&#39;. . In Deep Learning, prediction is often refered to as inference. I&#39;m going to load in my trained model for inference below. . learn_inf = load_learner(path/&#39;pokemon.pkl&#39;) . We can check that all seems correct by reviewing our potential output categories. . learn_inf.dls.vocab . (#150) [&#39;Abra&#39;,&#39;Aerodactyl&#39;,&#39;Alakazam&#39;,&#39;Alolan Sandslash&#39;,&#39;Arbok&#39;,&#39;Arcanine&#39;,&#39;Articuno&#39;,&#39;Beedrill&#39;,&#39;Bellsprout&#39;,&#39;Blastoise&#39;...] . Seems good! . First we create a label to guide the user. Next we create an Upload button so our user can submit their image. We then create a Classify button so the user to run the classification. Finally we create placeholders to display the uploaded image and the resulting classification. . After that we add an event handler to the Classify button to perform the necessary actions and populate the output placeholders on button click. . Finally, we place all the UI elements into a vertical box (VBox) to keep the UI looking clean. . user_inst = widgets.Label(&#39;Upload your Pokemon - Gen 1 only please!&#39;) btn_upload = widgets.FileUpload() btn_classify = widgets.Button(description=&#39;Classify&#39;) img_user = widgets.Output() lbl_pred = widgets.Label() def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) img_user.clear_output() with img_user: display(img.to_thumb(128,128)) pred, pred_idx, probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]*100:.02f}%.&#39; btn_classify.on_click(on_click_classify) VBox([user_inst,btn_upload,btn_classify,img_user,lbl_pred]) . The final step now is to turn this notebook into a real app. We will do this by using Voila and Binder. .",
            "url": "https://pdito.github.io/blog/deep%20learning/pokemon/2020/04/03/pokemon-gen1-classifier-blog.html",
            "relUrl": "/deep%20learning/pokemon/2020/04/03/pokemon-gen1-classifier-blog.html",
            "date": " â€¢ Apr 3, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Product Lead Data / Analytics @ FinTech. fast.ai/2020 student. Former: Trader, App Developer. Deep Learning, AI Enthusiast. Unofficial Autism Researcher. Find me on Twitter and Medium. .",
          "url": "https://pdito.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  
  

  
  

}