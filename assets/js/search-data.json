{
  
    
        "post0": {
            "title": "Multi-Label Classification with fastai2",
            "content": "Introduction . The first problem introduced in the fastai v2 course was that of bear classification. That is grizzly vs. black vs. teddy. . Initially we&#39;ll start with that example, using a typical multi-class classification approach. We&#39;ll then try to change our model using multi-label classification so we can handle the &#39;no bear&#39; case. . After that we&#39;ll introduce a similar, but harder problem, spending some time understanding what exactly our metrics mean. . Finally, we&#39;ll put it all together to tackle a true multi-label classification problem. . Thanks . Initially, I should acknowledge Deep Learning for Coders with fastai and PyTorch: AI Applications Without a PhD by Jeremy Howard and Sylvain Gugger, to which I have early access and forms the foundation of this post. . I&#39;d also like to thank all those in the fastai community who&#39;s various forum posts greatly aided my understanding here. . A special shout out to Zachary Mueller, who&#39;s multi-label example I have referenced here and who spent time directly answering my questions. He has a great YouTube resource on fastai v2. . Getting Started . We start with the usual imports for fastai. We also set our api keys for Microsoft Bing Image Search. For the purposes of this post they are hidden. We will not address the Bing API in any detail here; fastai provides us with a helper function, which you can view here. . Note: You can sign up for 7 days of Bing Image Search for free, with no need for a payment card. If you add a payment card, Microsoft offers a more permanent &#39;F1&#39; free tier that is more than sufficient for most non-production use cases. . from utils import * from fastai2.vision.widgets import * . key = &#39;xkey1x&#39; key2 = &#39;xkey2x&#39; . Next we define our three types of bear, and set our base path for our data set. We then loop through our bear types and download images from Bing. By default this is 150 images per query. . bear_types = &#39;grizzly&#39;,&#39;black&#39;,&#39;teddy&#39; path = Path(&#39;/home/pdito/AI/datasets/bears&#39;) . if not path.exists(): path.mkdir() for o in bear_types: dest = (path/o) dest.mkdir(exist_ok=True) results = search_images_bing(key, f&#39;{o} bear&#39;) download_images(dest, urls=results.attrgot(&#39;content_url&#39;)) . The next block of code gets a list of all the images in our bears data set, verifies each image, and deletes those that failed to download correctly. . fns = get_image_files(path) failed = verify_images(fns) failed.map(Path.unlink); . Let&#39;s take a look at our grizzly folder as an example. . (path/&#39;grizzly&#39;).ls() . (#143) [Path(&#39;/home/pdito/AI/datasets/bears/grizzly/00000088.jpg&#39;),Path(&#39;/home/pdito/AI/datasets/bears/grizzly/00000008.jpg&#39;),Path(&#39;/home/pdito/AI/datasets/bears/grizzly/00000034.jpg&#39;),Path(&#39;/home/pdito/AI/datasets/bears/grizzly/00000042.jpg&#39;),Path(&#39;/home/pdito/AI/datasets/bears/grizzly/00000027.jpg&#39;),Path(&#39;/home/pdito/AI/datasets/bears/grizzly/00000004.jpg&#39;),Path(&#39;/home/pdito/AI/datasets/bears/grizzly/00000030.jpg&#39;),Path(&#39;/home/pdito/AI/datasets/bears/grizzly/00000092.jpg&#39;),Path(&#39;/home/pdito/AI/datasets/bears/grizzly/00000052.jpg&#39;),Path(&#39;/home/pdito/AI/datasets/bears/grizzly/00000098.jpg&#39;)...] . The next step is to create our DataBlock and DataLoaders. You can read more about these in previous blog posts. Since to begin with we&#39;re dealing with a standard classification problem, we use a CategoryBlock. We use an 80/20 split for our train/validation data and we label our data using the parent folder name. . bears = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(128)) . dls = bears.dataloaders(path) . Let&#39;s view our data to make sure everything looks ok. . dls.train.show_batch(max_n=10, nrows=2) . All looks good. Next, let&#39;s create our Learner and train our model using the fine_tune approach. . learn = cnn_learner(dls, resnet18, metrics=accuracy) learn.fine_tune(4) . epoch train_loss valid_loss accuracy time . 0 | 1.584809 | 0.119369 | 0.965116 | 00:04 | . epoch train_loss valid_loss accuracy time . 0 | 0.270744 | 0.035475 | 0.976744 | 00:04 | . 1 | 0.205391 | 0.029300 | 0.976744 | 00:04 | . 2 | 0.144084 | 0.038638 | 0.988372 | 00:04 | . 3 | 0.107110 | 0.046285 | 0.988372 | 00:04 | . The results look good. 98.8% accuracy. But what happens when we try to predict on an image that contains neither a grizzly, black or teddy bear? Let&#39;s find out. . We begin grabbing an image of a bear and an image of a parrot. . im_bear = PILImage.create(&#39;bear.jpg&#39;) im_bear.show() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb2b628e450&gt; . im_parrot = PILImage.create(&#39;parrot.jpg&#39;) im_parrot.show() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb2d4783d10&gt; . Next we predict the class for each of these images using our current model. . learn.predict(im_bear)[0], learn.predict(im_parrot)[0] . (&#39;grizzly&#39;, &#39;teddy&#39;) . Our model is correct for the first bear image, it is a grizzly. But our parrot certainly isn&#39;t a teddy bear! Let&#39;s take a look at the numerical predictations. . learn.predict(im_parrot)[2] . tensor([0.0102182012, 0.0014118117, 0.9883699417]) . Not only does our model think the parrot is a teddy bear, it&#39;s 98.8% sure of it. . Pivoting to Multi-Label . In order to deal with the &#39;no bear&#39; situation, we have two options. We could grab random images that do not contain bears and train a new category for &#39;no bear&#39;. Alternatively, we can change our problem from a multi-class problem to a multi-label problem. This changes the model&#39;s behaviour from &#39;pick which class this image most likely belongs to&#39; to &#39;which of this list of classes appears in our image.&#39; In the latter case none is a reasonable response. . The beauty of this approach is that we do not require any additional data. . So let&#39;s set up our new DataBlock. We change from CategoryBlock to MultiCategoryBlock, which is our way of telling fastai this is a Multi-Label problem. This means we also need to change our labels, since MultiCategoryBlock expects a list of values for each image. We create a helper function, y_multilbl which simply converts our y labels from strings to lists of the singular string values. We use sklearn&#39;s Pipeline function which runs our labelling functions sequentially. . def y_multilbl(l): return [l] . bears = DataBlock( blocks=(ImageBlock, MultiCategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=Pipeline([parent_label, y_multilbl]), item_tfms=Resize(128)) . dls = bears.dataloaders(path) . Next, let&#39;s check to make sure everything looks ok. . dls.show_batch(max_n=10, nrows=2) . learn = cnn_learner(dls, resnet18, metrics=[partial(accuracy_multi, thresh=0.95)], loss_func=BCEWithLogitsLossFlat()) . BCEWithLogitsLossFlat . Now we have to create our learner, as illustrated above. A few changes here. First, let&#39;s talk about the loss function, BCEWithLogitsLossFlat. We don&#39;t actually need to specify this, since fastai would automatically do it for us, but it&#39;ll be useful later so we do so anyway. . BCEWithLogitsLossFlat is very similar to the loss function we used for the MNIST problem in a similar blog post, binary cross entropy. For the multi-class problem, we typically use negative log likelihood with log softmax (cross entropy), but this forces our probabilities to sum to 1 and forces us to greatly favour one class. In multi-label, this wouldn&#39;t make sense. For a start, we also need the ability to choose either more than one label or no label and secondly, we certainly don&#39;t want to push our model to favour any given label. . BCEWithLogitsLossFlat takes the sigmoid of our activations, and then applies the following logic: . -torch.where(targets==1, inputs, 1-inputs).log().mean() . In other words, where a given class exists in an image, take the log of the sigmoid of our activation, where it doesn&#39;t exist take the log of 1 - the sigmoid of our activation, then calculate the mean across all labels and finally take the negative. . Running through an example of this really helped our understanding. Let&#39;s grab one output from our model. . x,y = dls.train.one_batch() activs = learn.model(x) temp_x = activs[1] temp_y = y[1] temp_x, temp_y . (tensor([-0.3757612109, 1.2991540432, -3.7056896687], device=&#39;cuda:0&#39;, grad_fn=&lt;SelectBackward&gt;), tensor([0., 1., 0.], device=&#39;cuda:0&#39;)) . So we have output values of -0.375, -1.299 and -3.706 vs. expected labels of 0, 1, 0. Let&#39;s calculate our loss on this step by step. Firstly we calculate the sigmoid of our activations. This scales our values between 0 and 1. . temp_x_sig = temp_x.sigmoid() temp_x_sig . tensor([0.4071496129, 0.7856925726, 0.0239934195], device=&#39;cuda:0&#39;, grad_fn=&lt;SigmoidBackward&gt;) . Next, let&#39;s pass these results through the the initial part of the logic we mentioned earlier. . loss_stage_1 = torch.where(temp_y==1, temp_x_sig, 1-temp_x_sig) loss_stage_1 . tensor([0.5928503871, 0.7856925726, 0.9760065675], device=&#39;cuda:0&#39;, grad_fn=&lt;SWhereBackward&gt;) . Let&#39;s take the log and add the minus sign (it&#39;s easier to interpret the results after these steps). . Note: At this stage, we&#39;re almost complete with our loss calculation, the only remaining step is to calculate the mean across the labels, but first let&#39;s review our loss values label by label to help our understanding. . loss_stage_2 = -loss_stage_1.log() loss_stage_2 . tensor([0.5228132010, 0.2411896884, 0.0242859628], device=&#39;cuda:0&#39;, grad_fn=&lt;NegBackward&gt;) . Let&#39;s put everything together in a pandas DataFrame for easy comparison . results = torch.reshape(torch.cat([temp_x_sig, temp_y, loss_stage_2], 0),(3,3)) pd.DataFrame(results, index=[&#39;sigmoid&#39;,&#39;target&#39;,&#39;loss&#39;]) . 0 1 2 . sigmoid 0.407150 | 0.785693 | 0.023993 | . target 0.000000 | 1.000000 | 0.000000 | . loss 0.522813 | 0.241190 | 0.024286 | . Now they are presented nicely, making sense of these results is fairy straightforward. . For label 0, our target is 0 (ie. not present in image) however the sigmoid of our activation is reasonably high at 0.407, so we therefore have a pretty significant loss. . For label 1, our target is 1 (ie. present in image) and the sigmoid of our activation is 0.786, hence our loss is smaller in this case since we&#39;re predicting the correct result with reasonable confidence. . For label 2, our target is 0 (ie. not present in image) and the sigmoid is 0.02, hence our loss is ever smaller again. We&#39;re (correctly) predicting a very low chance of this label being present in the image. . It&#39;s crucial to understand how the negative of the log of the sigmoid activations vs targets leads to an actual loss number. Initiailly the signage may appear confusing, hopefully this breakdown helps. . Finally let&#39;s take the mean to get our overall loss. . loss_stage_3 = loss_stage_2.mean() loss_stage_3 . tensor(0.2627629638, device=&#39;cuda:0&#39;, grad_fn=&lt;MeanBackward0&gt;) . We can confirm this result by comparing to Pytorch&#39;s F.binary_cross_entropy_with_logits and fastai&#39;s BCEWithLogitsLossFlat - both of which are the same loss function (although fastai&#39;s has some additional benefits we don&#39;t use here). . F.binary_cross_entropy_with_logits(temp_x,temp_y) . tensor(0.2627629638, device=&#39;cuda:0&#39;, grad_fn=&lt;BinaryCrossEntropyWithLogitsBackward&gt;) . BCEWithLogitsLossFlat()(temp_x,temp_y) . tensor(0.2627629638, device=&#39;cuda:0&#39;, grad_fn=&lt;BinaryCrossEntropyWithLogitsBackward&gt;) . Accuracy_Multi . The other change in our Learner is the introduction of the accuracy_multi metric. On the multi-class classification problem, we could simply take the label with the highest probability and compare that to our target label to calculate the accuracy, however, in the case of multi-label that won&#39;t work since we may have more than 1 label prediction (or 0). . Instead we use the following: . return ((inp&gt;thresh)==targ.bool()).float().mean() . where inp is the sigmoid of our logits. This value is then compared to a specified threshold (which is 0.5 by default) to return a bool. These bools are compared to our targets for every label (in other words we get 1 if we match and 0 if not) and we can take a mean of the results to give our accuracy. . Say for example we had 5 labels in total. We run our model against an image with target of [1, 0, 0, 0, 1]. In other words label 0 and label 4 are present in the image, but labels 1, 2, 3 are not. The sigmoid of out logits are [0.7, 0.2, 0.1, 0.1, 0.4] and our threshold is 0.5. This makes our prediction [1, 0, 0, 0, 0]. So we&#39;re correct except for the case of label 4. We do not detect this label in the image based on our threshold, so we return 0, when we should return 1. In this case, our accuracy is the mean(1, 1, 1, 1, 0) == 80%. . Having looked at this metric in detail there are some further abstractions worth talking about, we&#39;ll do this later on in the post. For now, let&#39;s run our new model and see our results. . Back To Training . For the purposes of this example, we override the accuracy threshold to 0.95 for our metric. . learn.fit_one_cycle(4) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.832218 | 0.792952 | 0.872093 | 00:04 | . 1 | 0.567002 | 0.226285 | 0.980620 | 00:04 | . 2 | 0.428643 | 0.103618 | 0.984496 | 00:04 | . 3 | 0.345641 | 0.090847 | 0.976744 | 00:05 | . Our accuracy looks great, let&#39;s view some results to give us a better idea of what&#39;s going on. . learn.show_results() . Next let&#39;s run this model on our two test images again. . learn.predict(&#39;bear.jpg&#39;) . ((#1) [&#39;grizzly&#39;], tensor([False, True, False]), tensor([0.2130412012, 0.9998677969, 0.0050560534])) . learn.predict(&#39;parrot.jpg&#39;) . ((#1) [&#39;teddy&#39;], tensor([False, False, True]), tensor([0.3682020903, 0.0416439474, 0.9921609759])) . Not great, again, predicting a teddy for our parrot with a strong likelihood. How about a non-animal image, like a basketball? . im_ball = PILImage.create(&#39;ball.jpg&#39;) im_ball.show() . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb2b59a3590&gt; . learn.predict(&#39;ball.jpg&#39;) . ((#1) [&#39;teddy&#39;], tensor([False, False, True]), tensor([0.3023119271, 0.2594080865, 0.7907660604])) . Again, predicting teddy. However, looking at our sigmoid values, something seems strange. We&#39;re measuring accuracy vs. a threshold of 0.95, but our model is not using that threshold for our predictions. For teddy our prediction is 0.790766 which is &lt; 0.95. . Passing Threshold Into Predictions . Remember that by default, a threshold of 0.5 is used. For training, the threshold isn&#39;t relevant, since the loss function only cares about how strong our prediction is for a given label (as opposed to the binary correct or incorrect that our accuracy_multi metric cares about). . However, if we&#39;re using a certain threshold to assess our accuracy, it makes sense that this threshold should be use in our predictions. . Fastai v2 supports this out the box, but this is not something i&#39;ve come across on forums, blog posts, etc. . The key is in the loss function, BCEWithLogitsLossFlat(). Let&#39;s look at the code. . class BCEWithLogitsLossFlat(BaseLoss): &quot;Same as `nn.CrossEntropyLoss`, but flattens input and target.&quot; def __init__(self, *args, axis=-1, floatify=True, thresh=0.5, **kwargs): super().__init__(nn.BCEWithLogitsLoss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs) self.thresh = thresh def decodes(self, x): return x&gt;self.thresh . Above we can see that our loss function has a threshold parameter that is utilised by the decodes function. By default, learn.predict in fastai v2 has with_decoded=True. What this means it that when predicting our labels, our model will utilise the decodes method on our loss function (even though the loss function itself does not use it). . As stated in the documentation: with_decoded will return the decoded predictions using the decodes function of the loss function (if it exists). For instance, fastai&#39;s CrossEntropyFlat takes the argmax of predictions in its decodes. . So let&#39;s set this decodes threshold to the same value as our accuracy_multi and see how this impacts our prediction on the basketball image. . learn.loss_func = BCEWithLogitsLossFlat(thresh=0.95) . learn.predict(&#39;ball.jpg&#39;) . ((#0) [], tensor([False, False, False]), tensor([0.3023119271, 0.2594080865, 0.7907660604])) . We are now correctly predicting no label! Previously we had predicted teddy as the default threshold was 0.5, but since we&#39;ve now told our model to instead use a threshold of 0.95, we can see that we no longer return True for this class. . So that resolves the ball issue, but what about our parrot? A simple approach would be to increase our threshold to a very high amount. We&#39;ll retrain our model as we want to know how this impacts our accuracy on bears and not only on our parrot image. . learn = cnn_learner(dls, resnet18, metrics=[partial(accuracy_multi, thresh=0.995)], loss_func=BCEWithLogitsLossFlat(thresh=0.995)) learn.fine_tune(4) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.972547 | 0.208239 | 0.848837 | 00:04 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.335264 | 0.104634 | 0.903101 | 00:04 | . 1 | 0.261748 | 0.058515 | 0.980620 | 00:04 | . 2 | 0.206951 | 0.055292 | 0.984496 | 00:04 | . 3 | 0.164987 | 0.053522 | 0.980620 | 00:04 | . So it appears our accuracy remains pretty much unchanged, so we&#39;re doing well on our initial problem. Let&#39;s see if this threshold is enough to exclude our parrot. . learn.predict(&#39;parrot.jpg&#39;) . ((#0) [], tensor([False, False, False]), tensor([0.1208465919, 0.0334806517, 0.9793986082])) . Success, we&#39;re now predicting no bears in our parrot image. To confirm it&#39;s still working for bears, let&#39;s try with our bear image one last time. . learn.predict(&#39;bear.jpg&#39;) . ((#1) [&#39;grizzly&#39;], tensor([False, True, False]), tensor([0.0037450541, 0.9999380112, 0.0017104896])) . Still working. Excellent. . We can see however that an incredibly high threshold was required in order to prevent our parrot (that to a human eye appears nothing like a teddy bear) being classified as a bear. . Intuitively, we feel this will be a common problem for models where there are a low number of similar classes. We don&#39;t have the math to back that theory up, but it certainly feels like when a model&#39;s classes are incredibly specialised, it is more likely to be focusing on the niche differences between classes. In our case, it is likely not learning to recognise &#39;is this a bear&#39;, but more, &#39;what kind of bear is this&#39;? Given a teddy bear may be colourful and our parrot is colourful, we expect this is why our model likes to think our parrot is a teddy. . In order to test this theory, we&#39;ll first revisit the Oxford PETS dataset from a previous blog post. This data set has 37 classes. It is still somewhat specialised, but far less so than our three bear model. . A Harder Dataset - Oxford PETS . We&#39;ll set up our problem as usual, defining a DataBlock, using that to build our DataLoaders and set up our Learner following a similar structure to above. We&#39;ll use a default thresh=0.5 to get started. . path = untar_data(URLs.PETS)/&#39;images&#39; pets_multi = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), get_items=get_image_files, splitter=RandomSplitter(), get_y=Pipeline([RegexLabeller(pat = r&#39;/([^/]+)_ d+.jpg$&#39;), multi_l]), item_tfms=RandomResizedCrop(460, min_scale=0.75), batch_tfms=[*aug_transforms(size=224, max_warp=0), Normalize.from_stats(*imagenet_stats)]) dls = pets_multi.dataloaders(untar_data(URLs.PETS)/&quot;images&quot;, bs=64) . learn = cnn_learner(dls, resnet34, metrics=[partial(accuracy_multi, thresh=0.5)], loss_func=BCEWithLogitsLossFlat(thresh=0.5)) . learn.fit_one_cycle(4) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.907477 | 0.585815 | 0.703891 | 00:16 | . 1 | 0.423479 | 0.107985 | 0.989430 | 00:16 | . 2 | 0.154661 | 0.050601 | 0.989138 | 00:16 | . 3 | 0.092592 | 0.045708 | 0.989522 | 00:16 | . We&#39;re achieving a very good result on our metrics, let&#39;s take a look at some results. . learn.show_results() . Since we&#39;re working with a multi-label problem, no label is a perfectly reasonable prediction, and we can see that we are predicting that for a couple of classes above. . Let&#39;s see what happens when we try to predict on our ball and parrot images again. . learn.predict(&#39;ball.jpg&#39;) . ((#0) [], tensor([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]), tensor([0.0344677567, 0.0289024077, 0.0157086626, 0.0887096450, 0.0291722007, 0.0225640945, 0.0303591248, 0.0249853153, 0.0220691841, 0.0356397890, 0.0203688014, 0.0535237640, 0.0143291624, 0.0434767380, 0.0174501762, 0.0574767962, 0.0358610377, 0.0763304904, 0.0220106784, 0.0296993256, 0.0434434600, 0.0155749880, 0.1074422523, 0.0555117168, 0.0557364561, 0.1804087758, 0.0267083216, 0.0770314708, 0.0588480048, 0.0113218101, 0.0045709992, 0.0047294935, 0.0619068146, 0.1323680580, 0.0307631791, 0.0234820042, 0.0313019603])) . learn.predict(&#39;parrot.jpg&#39;) . ((#0) [], tensor([False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False]), tensor([0.0272999443, 0.0280438587, 0.0219808687, 0.0182054788, 0.0478428788, 0.0156985801, 0.0477190539, 0.0337769985, 0.0347514637, 0.0734078810, 0.0258022957, 0.0419126935, 0.0284733139, 0.0186693668, 0.0259662066, 0.0571308024, 0.0251882412, 0.0126766358, 0.0200468618, 0.1058133170, 0.0308309682, 0.0275608227, 0.0092588365, 0.0612633899, 0.0365996957, 0.0430336297, 0.0188603010, 0.0152278058, 0.0090585034, 0.0186921172, 0.0312294867, 0.0244376026, 0.0069209770, 0.0454699658, 0.0412447229, 0.1049316525, 0.0231012963])) . Our intuition seems solid, even with a far lower threshold, we still don&#39;t classify our random images. Let&#39;s see how close we came to doing so by looking at the largest sigmoid value associated with a class. . learn.predict(&#39;parrot.jpg&#39;)[2].max() . tensor(0.1058133170) . Nowhere near our threshold. So here we appear to have a much more robust model. A strong accuracy metric despite a reasonably high threshold, and (after a very brief test!) no tendency to classify images from outside the model&#39;s vocab. . Finally, let&#39;s try on our bear image, since a bear has more visual similarity to dogs and cats than a ball or parrot. . learn.predict(&#39;bear.jpg&#39;)[0], learn.predict(&#39;bear.jpg&#39;)[2].max() . ((#0) [], tensor(0.0914672017)) . Here too, we predict that this image does not contain any class in our model&#39;s vocab and the closest it came to doing so was with a value of 0.091, well below our threshold. . Interpreting Results . Along with the topic of using learn.predict for a specified threshold, I also struggled to find many resources on interpreting your results for the multi-label problem. Fastai2&#39;s library contains a useful nbs (notebooks) folder, and browsing through there I did find an example which i&#39;ll repeat below for reference. . interp = Interpretation.from_learner(learn) interp.plot_top_losses(6) . target predicted probabilities loss . 0 saint_bernard | chihuahua | tensor([0.0191773269, 0.0291777253, 0.0287358314, 0.0407794043, 0.0160873029, 0.0102614323, 0.0138118779, 0.0264858454, 0.0237598792, 0.0112322308, 0.0332457609, 0.0126576060, 0.0594058782, n 0.0228471495, 0.0208546109, 0.0410828702, 0.0311073419, 0.6115643382, 0.0298188180, 0.0383781083, 0.0240385681, 0.0346213393, 0.0620193258, 0.1795209050, 0.0100610657, 0.0418684185, n 0.0482454076, 0.0119415484, 0.0869454592, 0.0289356913, 0.0069528795, 0.0065667992, 0.0102934241, 0.0281963460, 0.0165728200, 0.0130003141, 0.0439361744]) | 0.1921067088842392 | . 1 British_Shorthair | | tensor([0.0169720743, 0.0048210481, 0.0097904988, 0.0115108946, 0.0074902344, 0.0224394370, 0.0089336643, 0.0055326088, 0.0257332828, 0.0552611127, 0.0085214451, 0.0111031597, 0.0034427270, n 0.0086471168, 0.0220964868, 0.0132461423, 0.0258738659, 0.0223778225, 0.0188840032, 0.0208843183, 0.0754167736, 0.0060531111, 0.0192722268, 0.0156424437, 0.0861392766, 0.0046086055, n 0.0131388670, 0.0058022072, 0.0222118516, 0.0085063148, 0.0061178822, 0.0038811087, 0.0136336358, 0.0113776382, 0.0126673467, 0.0086364420, 0.0523484312]) | 0.15105541050434113 | . 2 staffordshire_bull_terrier | great_pyrenees | tensor([0.0236374903, 0.0493448600, 0.0683457777, 0.0553755648, 0.0567676835, 0.0545176901, 0.0214479323, 0.0388006121, 0.0643670484, 0.0489204153, 0.0138649782, 0.0260447077, 0.1183850467, n 0.0807636008, 0.0479573421, 0.0223606993, 0.0297511574, 0.0389028899, 0.0579909571, 0.0639621541, 0.0493633971, 0.5388699770, 0.0759820417, 0.0245810952, 0.0594581850, 0.0590638034, n 0.0093539329, 0.0323737226, 0.0158522259, 0.0208221190, 0.0374304652, 0.4657154381, 0.1278894842, 0.2817675173, 0.1086672544, 0.0211554207, 0.0366526656]) | 0.15010440349578857 | . 3 Persian | | tensor([0.0215264726, 0.0199572667, 0.0497998521, 0.0502034724, 0.0313545242, 0.0809872001, 0.0802563578, 0.0177070722, 0.0188304204, 0.0685411319, 0.0212426130, 0.0176774804, 0.0132121751, n 0.0662501380, 0.0144032445, 0.0171497706, 0.0121534318, 0.0313094743, 0.0751761049, 0.0483469926, 0.0381074771, 0.0269840118, 0.0417266563, 0.0279905125, 0.0334162526, 0.0160706397, n 0.0078639621, 0.0429174788, 0.0107957693, 0.0501838662, 0.0123773944, 0.0138175488, 0.0504349507, 0.0227040108, 0.0135195516, 0.0098678749, 0.0756983832]) | 0.14318469166755676 | . 4 american_bulldog | | tensor([0.0110149784, 0.0280855913, 0.0167586915, 0.0069804704, 0.0092811221, 0.0165379923, 0.0081878593, 0.0049462370, 0.0236487668, 0.0098384554, 0.0052051614, 0.0028247987, 0.0155410198, n 0.0412584916, 0.0329408273, 0.0449480340, 0.0579465330, 0.0094725322, 0.0130668366, 0.0438410081, 0.0132257836, 0.0107391635, 0.0170740709, 0.0173012149, 0.0236651823, 0.0090893013, n 0.0120717762, 0.0090606315, 0.0053765010, 0.0061920220, 0.0632782727, 0.0112180524, 0.0098168915, 0.0426992513, 0.1416785568, 0.0058866274, 0.0061665629]) | 0.1345226764678955 | . 5 miniature_pinscher | chihuahua | tensor([0.0391823798, 0.0340512469, 0.0236209221, 0.0346271433, 0.0163501874, 0.0153648732, 0.0279628895, 0.0324261412, 0.0152906226, 0.0263517126, 0.0465812497, 0.0189483371, 0.0297061373, n 0.0078280307, 0.0135169122, 0.0189277250, 0.0265566763, 0.5460162163, 0.0591060445, 0.0394829996, 0.0114016440, 0.0185023416, 0.0168374609, 0.0301355366, 0.0348688252, 0.0498505682, n 0.0466404669, 0.0165892262, 0.0329290703, 0.0310162418, 0.0068383408, 0.0131168514, 0.0112008490, 0.0337499082, 0.0450439528, 0.0274948552, 0.0517130494]) | 0.13050369918346405 | . A lot harder to interpret than with the multi-class problem. It does appear our model may understandably be struggling a little on images where the contrast between the pet and the background is limited. . I&#39;m sure we could expand out these results further, and glean some useful information, but that is not within the scope of this blog post. . Understanding Accuracy_Multi . Something I struggled with considerably was our accuracy_multi metric, more concretely, why it appeared to be so high. In our previous blog post we struggled to achieve 95% accuracy on the PETS dataset, yet here with barely any training, our accuracy_multi exceeds 98%. . Why is this? Could multi-label simply be a much more powerful approach? Unfortunately the answer is no, it&#39;s all in the math. . Take an example where you have 100 images in your validation set, evenly split across 10 categories, so 10 images per category. Let&#39;s say our models (both multi-class and multi-label) are almost perfect, however, there is 1 category they always get wrong. . Multi-Class Our accuracy for our multi-class model is 90 / 100 = 90%. . Multi-Label Our multi-label model is correct for 90 / 100 classes, however, for the classes where it is wrong, we use every label to calculate the accuracy. Let&#39;s assume in this model we always misclassify the troublesome class by predicting the wrong class instead. We correctly predict the absense of any of the other classes. . So for 10 of the images where we are not perfectly correct, our loss will look like this (1, 1, 1, 1, 1, 1, 1, 1, 0, 0).mean() = 0.8 = 80%. So we even though we are not detecting a class that exists in the image (which is one of the 0s) and we are detecting a class that does not exist in the image (which is the other 0) we are still correct in not predicting the prescene of the other 8 classes. . So for our overall accuracy_multi metric, we have ((90 1) + (10 0.8)) / 100 = 98%. . You can see how the accuracy_multi is a far more generous metric, particularly with the problems we have tackled so far here where our images do not have multiple labels. . The conclusion here is that accuracy and accuracy_multi are not comparable. . True Multi-Cat Data Set . For a blog post about multi-label, it probably makes sense to train a model on a genuine multi-label data set, in this case, we use the PASCAL 2007 dataset. Thank you to the authors and contributors. . path = untar_data(URLs.PASCAL_2007) path.ls() . (#8) [Path(&#39;train&#39;),Path(&#39;test&#39;),Path(&#39;segmentation&#39;),Path(&#39;train.csv&#39;),Path(&#39;test.json&#39;),Path(&#39;train.json&#39;),Path(&#39;valid.json&#39;),Path(&#39;test.csv&#39;)] . For this dataset, it looks like our labels are stored in a CSV file, let&#39;s load that CSV into a Pandas dataframe and take a look. . df = pd.read_csv(path/&#39;train.csv&#39;) df.head() . fname labels is_valid . 0 000005.jpg | chair | True | . 1 000007.jpg | car | True | . 2 000009.jpg | horse person | True | . 3 000012.jpg | car | False | . 4 000016.jpg | bicycle | True | . So the CSV returns a string with space separate labels for each image, and also tells us if that image should be used in the validation set (as opposed to the training set). . Let&#39;s start by creating a function to get the path to our images, what we otherwise refer to as our x. We reference our fname column. We can see above this just gives us the filename, so we need to prepend the path ahead of this. . def get_x(row): return path/&#39;train&#39;/row[&#39;fname&#39;] . dblock = DataBlock(get_x = get_x) dsets = dblock.datasets(df) dsets.train[1] . (Path(&#39;train/009532.jpg&#39;), fname 009532.jpg labels car person is_valid True Name: 4792, dtype: object) . The output here is x, y. So our x looks good, now we need to work on our y. Our labels are stored in the labels column, which we can reference into. There is one more step however, as our labels are provided as one long string and our model expects a list. We can use Python&#39;s string split function which splits a string sentence into a list of words. . def get_y(row): return row[&#39;labels&#39;].split(&#39; &#39;) . dblock = DataBlock(get_x = get_x, get_y = get_y) dsets = dblock.datasets(df) dsets.train[1] . (Path(&#39;train/006950.jpg&#39;), [&#39;person&#39;, &#39;bicycle&#39;]) . It now looks like we&#39;re grabbing our image paths and corresponding labels correctly. The final step is to use the is_valid column to split our training/validation set. Previously fastai had a function that would do that for us, split_from_df but this has not be adopted by fastai v2 yet. . It&#39;s pretty simple to write our own. Pandas index function allows us to return indexes in to our dataframe, for a column of bools or ints, which we already have in the is_valid column. . Note: in Pandas ~ means bitwise not / inverse of boolean mask. . df.index[~df[&#39;is_valid&#39;]], df.index[df[&#39;is_valid&#39;]] . (Int64Index([ 3, 5, 9, 11, 13, 14, 15, 16, 17, 20, ... 4991, 4993, 4996, 4998, 4999, 5000, 5001, 5004, 5009, 5010], dtype=&#39;int64&#39;, length=2501), Int64Index([ 0, 1, 2, 4, 6, 7, 8, 10, 12, 18, ... 4992, 4994, 4995, 4997, 5002, 5003, 5005, 5006, 5007, 5008], dtype=&#39;int64&#39;, length=2510)) . splitter in our DataBlock expects two lists of integers, so we convert these outputs from Pandas Int64Index to lists, using the to_list() function. . def splitter(df): train = df.index[~df[&#39;is_valid&#39;]].to_list() valid = df.index[df[&#39;is_valid&#39;]].to_list() return train,valid . Let&#39;s test it all out together. . dblock = DataBlock(get_x = get_x, get_y = get_y, splitter=splitter) dsets = dblock.datasets(df) dsets.train[0], len(dsets.train), len(dsets.valid) . ((Path(&#39;train/000012.jpg&#39;), [&#39;car&#39;]), 2501, 2510) . All looks fine. The size of our training and validation sets tie in to the Int64Index lengths we found above. Now we can create our final DataBlock. . dblock = DataBlock(blocks=(ImageBlock, MultiCategoryBlock), splitter=splitter, get_x=get_x, get_y=get_y, item_tfms = RandomResizedCrop(224, min_scale=0.90)) dls = dblock.dataloaders(df) . Let&#39;s now visualise some data for a batch to make sure the labels tie into the image and everything looks ok. . dls.show_batch(nrows=1, ncols=5) . All looks good, so we&#39;re ready to create our Learner and train our model. . learn = cnn_learner(dls, resnet50, metrics=partial(accuracy_multi, thresh=0.2)) learn.fine_tune(3, base_lr=3e-3, freeze_epochs=4) . epoch train_loss valid_loss accuracy_multi time . 0 | 0.918062 | 0.656770 | 0.268227 | 00:12 | . 1 | 0.780111 | 0.525478 | 0.324064 | 00:12 | . 2 | 0.559694 | 0.177679 | 0.840100 | 00:13 | . 3 | 0.318779 | 0.092976 | 0.957669 | 00:13 | . epoch train_loss valid_loss accuracy_multi time . 0 | 0.093386 | 0.090855 | 0.962848 | 00:16 | . 1 | 0.074832 | 0.084515 | 0.968865 | 00:16 | . 2 | 0.053708 | 0.080952 | 0.970558 | 00:16 | . 97% accuracy, pretty solid. Let&#39;s visualise our results. . learn.show_results() . What Threshold? . To improve our accuracy, we have all the normal options. More epochs, deeper network, data augmentation, learning rate tuning, etc. But what about finetuning our threshold? . If our threshold is too low, we&#39;ll be overconfident on a label being present in an image. If our threshold is too high, we&#39;ll be overcautious on a label being present in an image. . So clearly getting the threshold correct will have a huge impact on our model&#39;s performance on our metric. . We can actually take the predictions and targets from our model and manually run these against different values for threshold to see how this influences our metric. . torch.linspace allows us generate equally distributed numbers between two values (across a specified number of steps) which we can then pass into our accuracy_multi function. . Note: By default get_preds applies sigmoid for us, so we&#39;ll need to tell accuracy_multi not to, by specifying sigmoid=False. . preds,targs = learn.get_preds() xs = torch.linspace(0.05,0.995,100) accs = [accuracy_multi(preds, targs, thresh=i, sigmoid=False) for i in xs] plt.plot(xs,accs); . So we can see here that the best threshold is actually closer to 0.4. But aren&#39;t we breaking the golden rule here, fitting to our validation set? . Apparently not. According to the book, this is where theory and practice can collide. We note our plot above is a very smooth curve, and the takeaway here is that given that&#39;s the case &#39;we&#39;re clearly not picking some inappropriate outlier.&#39; . Final Thoughts . Overall, pretty surprising how easy it is to set up a multi-label classification model, particularly wtih fastai doing a lot of the heavy lifting for us. . The multi-label approach appears a valid technique for adding the &#39;no class is present&#39; output to a multi-class problem without the need for additional data. . Finally, we are curious about different metrics for measuring multi-label performance. Of course, this depends on the problem we are tackling, but for many of the use cases above, accuracy_multi feels like it may not be the best option. This is particularly the case where your images have lots of potential labels, but the average labels-per-image number is low (in this case you can get good accuracy_multi simply by predicting 0 for everything). .",
            "url": "https://pdito.github.io/blog/deep%20learning/fastai/fastai2/multicat/multi/label/classification/2020/05/27/multi-cat-fastai2.html",
            "relUrl": "/deep%20learning/fastai/fastai2/multicat/multi/label/classification/2020/05/27/multi-cat-fastai2.html",
            "date": " • May 27, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "State of the Art Performance with fastai on the Oxford-IIT Pet Dataset",
            "content": "The Project . Here we experiment with fastai v2 to optimise performance on the Oxford-IIT Pet Dataset. We&#39;ll look at transfer learning with finetuning, learning rate optimisation including the one cycle policy, image augmentation, epochs and a few other tricks. . According to papers with code, the current state of the art performances on this dataset are all from last year (2019). . BiT-L 96.76% | EfficientNet-B7 95.40% | FixSENet-154 94.8% | We&#39;ll see how we can stack up against these, although note: the dataset provides no test set, so the results whilst comparable, may not line up exactly. . The Dataset . We&#39;re going to working with the Oxford-IIT Pet Dataset, a 37 category dataset of pets (cats and dogs), with around 200 images for each class. . Thanks to O. M. Parkhi, A. Vedaldi, A. Zisserman, C. V. Jawahar for sharing this dataset with the world. You can find the original link here. . Getting Started . To begin, we import the usual libraries and download our data using the fastai convenience class UNTAR_DATA. We also set Path.BASE_PATH to tidy up our output (displayed relative to our path, rather than with the full path). . from utils import * from fastai2.vision.widgets import * path = untar_data(URLs.PETS) Path.BASE_PATH = path path.ls() . (#2) [Path(&#39;annotations&#39;),Path(&#39;images&#39;)] . Lets navigate to the images folder and see what our data looks like. . (path/&#39;images&#39;).ls() . (#7394) [Path(&#39;images/British_Shorthair_165.jpg&#39;),Path(&#39;images/Abyssinian_67.jpg&#39;),Path(&#39;images/american_pit_bull_terrier_182.jpg&#39;),Path(&#39;images/pomeranian_84.jpg&#39;),Path(&#39;images/Ragdoll_145.jpg&#39;),Path(&#39;images/japanese_chin_54.jpg&#39;),Path(&#39;images/american_pit_bull_terrier_98.jpg&#39;),Path(&#39;images/english_setter_93.jpg&#39;),Path(&#39;images/Siamese_257.jpg&#39;),Path(&#39;images/Sphynx_213.jpg&#39;)...] . It appears all our data is in the same folder, and categorised by file name. We&#39;ll need to use a regex expression to extract the class name from the file name, particularly since we don&#39;t care about the series number, and some class names contain more than one word. . I knew nothing about Regex when I started DL, but found a couple of excellent tutorials here and here that really helped. . Next, let&#39;s get our Dataset into a basic DataBlock and use that to initialise our Dataloaders. We use a RandomSplitter with seed for our train/valid split, to keep that split consistent when we make changes to our DataBlock later. . petsdata = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items = get_image_files, splitter = RandomSplitter(seed=42), get_y = using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(224) ) dls = petsdata.dataloaders(path/&#39;images&#39;) . Let&#39;s take a look at both a list of our classes and a sample of a mini-batch to make sure everything looks ok. . dls.vocab . (#37) [&#39;Abyssinian&#39;,&#39;Bengal&#39;,&#39;Birman&#39;,&#39;Bombay&#39;,&#39;British_Shorthair&#39;,&#39;Egyptian_Mau&#39;,&#39;Maine_Coon&#39;,&#39;Persian&#39;,&#39;Ragdoll&#39;,&#39;Russian_Blue&#39;...] . dls.show_batch(nrows=1, ncols=5) . The data looks good, so let&#39;s create a cnn_learner which provides us with a suitable model for transfer learning in vision problems. . For the purposes of this post, we start with pretrained=False, but normally you would omit this line. Likewise, we specify our loss_func for completeness, but fastai is smart enough to do this for us. . Finally, we fit across 5 epochs. . learn = cnn_learner(dls, resnet34, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit(5) . epoch train_loss valid_loss accuracy time . 0 | 4.285869 | 4.165880 | 0.079161 | 00:19 | . 1 | 3.980685 | 4.139607 | 0.077131 | 00:19 | . 2 | 3.731223 | 3.892704 | 0.084574 | 00:19 | . 3 | 3.701278 | 11.792640 | 0.060217 | 00:19 | . 4 | 3.639907 | 3.554760 | 0.087957 | 00:19 | . Ok, so, we&#39;re learning, but clearly at a minimum we&#39;re going to need a bunch more epochs. However, since we&#39;re using ResNet34 that has already been trained on ImageNet for 1000s of epochs, we can probably benefit from initialising with the pre-trained weights. . Transfer Learning &amp; Finetuning . Since pretrainedis true by default, we simply omit this parameter when we create our learner. . learn = cnn_learner(dls, resnet34, loss_func=F.cross_entropy, metrics=accuracy) learn.fit(5) . epoch train_loss valid_loss accuracy time . 0 | 0.784199 | 0.296528 | 0.897158 | 00:13 | . 1 | 0.420405 | 0.242651 | 0.924222 | 00:14 | . 2 | 0.292644 | 0.238782 | 0.922192 | 00:14 | . 3 | 0.233687 | 0.242544 | 0.922869 | 00:14 | . 4 | 0.185453 | 0.252115 | 0.920839 | 00:14 | . A huge improvement. We&#39;ve jumped from 9% accuracy to 92% accuracy. There is however a better technique for fine tuning our pretrained model for our own problem. . By default, our model is frozen, with the exception of its head. What this means is, when we are training, we are only updating our weights for the latter part of the model (the layers which were created for our specific dependent variables). . This is best illustrated by looking at the summary of our learner. . learn.summary() . Sequential (Input shape: [&#39;64 x 3 x 224 x 224&#39;]) ================================================================ Layer (type) Output Shape Param # Trainable ================================================================ Conv2d 64 x 64 x 112 x 112 9,408 False ________________________________________________________________ BatchNorm2d 64 x 64 x 112 x 112 128 True ________________________________________________________________ ReLU 64 x 64 x 112 x 112 0 False ________________________________________________________________ MaxPool2d 64 x 64 x 56 x 56 0 False ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ ReLU 64 x 64 x 56 x 56 0 False ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ ReLU 64 x 64 x 56 x 56 0 False ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ ReLU 64 x 64 x 56 x 56 0 False ________________________________________________________________ Conv2d 64 x 64 x 56 x 56 36,864 False ________________________________________________________________ BatchNorm2d 64 x 64 x 56 x 56 128 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 73,728 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ ReLU 64 x 128 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 8,192 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ ReLU 64 x 128 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ ReLU 64 x 128 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ ReLU 64 x 128 x 28 x 28 0 False ________________________________________________________________ Conv2d 64 x 128 x 28 x 28 147,456 False ________________________________________________________________ BatchNorm2d 64 x 128 x 28 x 28 256 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 294,912 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 32,768 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ ReLU 64 x 256 x 14 x 14 0 False ________________________________________________________________ Conv2d 64 x 256 x 14 x 14 589,824 False ________________________________________________________________ BatchNorm2d 64 x 256 x 14 x 14 512 True ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 1,179,648 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ ReLU 64 x 512 x 7 x 7 0 False ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 131,072 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ ReLU 64 x 512 x 7 x 7 0 False ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ ReLU 64 x 512 x 7 x 7 0 False ________________________________________________________________ Conv2d 64 x 512 x 7 x 7 2,359,296 False ________________________________________________________________ BatchNorm2d 64 x 512 x 7 x 7 1,024 True ________________________________________________________________ AdaptiveAvgPool2d 64 x 512 x 1 x 1 0 False ________________________________________________________________ AdaptiveMaxPool2d 64 x 512 x 1 x 1 0 False ________________________________________________________________ Flatten 64 x 1024 0 False ________________________________________________________________ BatchNorm1d 64 x 1024 2,048 True ________________________________________________________________ Dropout 64 x 1024 0 False ________________________________________________________________ Linear 64 x 512 524,288 True ________________________________________________________________ ReLU 64 x 512 0 False ________________________________________________________________ BatchNorm1d 64 x 512 1,024 True ________________________________________________________________ Dropout 64 x 512 0 False ________________________________________________________________ Linear 64 x 37 18,944 True ________________________________________________________________ Total params: 21,830,976 Total trainable params: 563,328 Total non-trainable params: 21,267,648 Optimizer used: &lt;function Adam at 0x7fd01587e3b0&gt; Loss function: &lt;function cross_entropy at 0x7fd029079b90&gt; Model frozen up to parameter group number 2 Callbacks: - TrainEvalCallback - Recorder - ProgressCallback . The key output here is: . Total params: 21,830,976 Total trainable params: 563,328 Total non-trainable params: 21,267,648 . We can see in its current state our learner is only training a small subset of our parameters. . Above we can see that this did give good results and this instincitvely makes sense. The head of our model is new, with random weights, whilst the remainder of the model is pretrained and already understands image classification (detecting edges, shapes, patterns, etc). . Clearly we want to initially focus our training on the head of the model. However, in order to get a little bit more performance, perhaps we want to unfreeze our entire model later so it can specialise on our specific problem. . fastai gives us a simple way to do this, fine_tune. We can pass this epochs for both frozen and unfrozen training, as demonstrated below. In this case we train our frozen model for 5 epochs as earlier, but then train the entire unfrozen model for 3 additional epochs. . learn = cnn_learner(dls, resnet34, pretrained=True, loss_func=F.cross_entropy, metrics=accuracy) learn.fine_tune(3, freeze_epochs=5) . epoch train_loss valid_loss accuracy time . 0 | 2.988474 | 0.912918 | 0.762517 | 00:13 | . 1 | 1.133766 | 0.353341 | 0.886333 | 00:13 | . 2 | 0.571935 | 0.313648 | 0.894452 | 00:14 | . 3 | 0.406759 | 0.324028 | 0.885656 | 00:14 | . 4 | 0.315896 | 0.308718 | 0.902571 | 00:14 | . epoch train_loss valid_loss accuracy time . 0 | 0.290366 | 0.379901 | 0.893099 | 00:19 | . 1 | 0.229464 | 0.269846 | 0.914073 | 00:19 | . 2 | 0.097278 | 0.247730 | 0.923545 | 00:19 | . A small performance bump, but in reality, we probably need more epochs to fully benefit from this methodolody. In addition, equally as important is learning rate optimisation. Until now, we&#39;ve used the defaults built in to the fastai library, but we can use tools to help us find the optimal rate. . Learning Rate Optimisation . In a method originally described by Leslie Smith in his 2015 paper, gradually increasing our learning rate and plotting the loss can help us select an optimal learning rate. . Note: this rate will change as the model learns, and will change dramatically once the model is unfrozen. . Fastai has built in functionality to do this, calling lr_find() on our learner. . learn = cnn_learner(dls, resnet34, loss_func=F.cross_entropy, metrics=accuracy) learn.lr_find() . SuggestedLRs(lr_min=0.00831763744354248, lr_steep=0.0030199517495930195) . Interpretation of this chart is more an art than a science, but common approaches are to select the point at which the downwards slope is the steepest or alternatively select the point at which the curve is at its minimum, and divide that value by 10. . In this case, the slope is steepest at 0.003 (3e-3) and at its minimum at 0.08, which divided by 10 gives us 0.008 (8e-3). . Let&#39;s try the prior. . learn = cnn_learner(dls, resnet34, loss_func=F.cross_entropy, metrics=accuracy) learn.fine_tune(2, base_lr=3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.275583 | 0.318984 | 0.897158 | 00:14 | . epoch train_loss valid_loss accuracy time . 0 | 0.501107 | 0.409889 | 0.875507 | 00:19 | . 1 | 0.241149 | 0.241694 | 0.922192 | 00:19 | . After only 3 epochs and less than one minute of training we&#39;re able to match our best accuracy so far. . One Cycle Policy . Something we haven&#39;t mentioned so far is the One Cycle training policy that is currently the preferred approach for training. This approach, again a discovery of Leslie Smith in his 2018 paper, has been adopted by the fastai framework. . This basic principle is that we start off training with a low learning rate, increasing that rate in a linear fashion until we are around halfway through our epochs. From there, we reverse the direction gradually decreasing our learning rate, again, in a linear fashion. In addition, the final few epochs are often trained at an even lower learning rate. . Typically we use the learning rate finder to determine our maximum rate, dividing that rate by 10 to get our minimum rate. . We use learn.fit_one_cycle() passing our maximum learning rate using lr_max. From my experience the one cycle policy does make the training appear more volatile, however the end result is almost universally better. . For those who want a deeper dive, Sylvain Gugger has an excellent post. . Let&#39;s run a few epochs on our previous example. . learn = cnn_learner(dls, resnet34, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(3, lr_max=3e-3) . epoch train_loss valid_loss accuracy time . 0 | 1.099183 | 0.361369 | 0.886333 | 00:13 | . 1 | 0.466258 | 0.266645 | 0.914750 | 00:13 | . 2 | 0.239239 | 0.230401 | 0.926252 | 00:14 | . Even higher accuracy here, 92.6% again after just 3 epochs. Also worth noting the speed of the training, just 40 seconds in total. . An additional benefit of using this method, is that we can now unfreeze our model, re-run our learning rate finder, and run the one cycle policy again on all our parameters, but with a more appropriate learning rate. . learn.save(&#39;pets-foc-3epochs-3e-3&#39;) learn.unfreeze() learn.lr_find() . SuggestedLRs(lr_min=7.585775847473997e-08, lr_steep=1.3182567499825382e-06) . Notice the chart is completely different to before. In this case, its pretty difficult to accurately pick a good rate. As a rule of thumb, we want to choose a learning rate well before the loss suddenly jumps. In this case the loss begins to climb sharply at 1e-3, so we move back significantly from that and opt for 1e-5. This process may require some trial and error, which is why we saved our weights above. . learn.fit_one_cycle(6, lr_max=1e-5) . epoch train_loss valid_loss accuracy time . 0 | 0.191230 | 0.233795 | 0.931664 | 00:19 | . 1 | 0.169193 | 0.230405 | 0.932341 | 00:19 | . 2 | 0.147155 | 0.225354 | 0.937754 | 00:19 | . 3 | 0.115199 | 0.222786 | 0.938430 | 00:19 | . 4 | 0.098137 | 0.217740 | 0.937077 | 00:19 | . 5 | 0.106159 | 0.219465 | 0.937077 | 00:19 | . In this case, it looks like our selected rate was reasonable. Another solid increase in performance, now 93.7% accuracy. . Discriminative Learning Rates . Previously we&#39;ve worked with two binary cases. Firstly, a frozen model where we are only training parameters in our model&#39;s head. Secondly, an unfrozen model where we are training all the parameters. In these cases our training loop used the same lr_maxin every layer. . Discriminative Learning, an idea originated by this Jason Yosinski paper, lets us take advantage of the fact that the parameters in the early layers in the model likely need lower learning rates than the latter layers. Remember the early layers often detect features that are universal to most vision problems (edges, patterns, circles, etc), whereas the latter laters become more specialised towards the problem we are trying to solve. . Fastai allows us to pass a Python sliceanywhere a learning rate is expected. In this case the first layer would be passed the lowest learning rate during training, the last layer would be passed the highest learning rate during training with everything else in between an interpolated value between the two based on the position of the layer. . Given our previous lr_max was 1e-5, let&#39;s try again, but this time with a slice centered around that value of 1e-6 to 1e-4. . learn.load(&#39;pets-foc-3epochs-3e-3&#39;) learn.fit_one_cycle(6, lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss accuracy time . 0 | 0.189376 | 0.226536 | 0.932341 | 00:18 | . 1 | 0.176528 | 0.225593 | 0.937077 | 00:19 | . 2 | 0.142633 | 0.218502 | 0.935047 | 00:19 | . 3 | 0.113348 | 0.215389 | 0.940460 | 00:19 | . 4 | 0.092376 | 0.213221 | 0.937077 | 00:19 | . 5 | 0.091594 | 0.209560 | 0.940460 | 00:19 | . Now up to 94.0% accuracy, which seems like a small improvement, but at this stage even small improvements are significant. . Deeper Architectures . Until now we&#39;ve been using ResNet34, a 34 layer neural net. Deeper networks with more parameters are typically able to model our data more accurately. . It&#39;s worth noting that these improvements can be incredibly useful, such as learning real generalisable features and relationships within our data, but they can also represent overfitting, where our model learns features and relationships that are overly specific to our training data. There are also other costs to consider such as longer training times, and more strenuous hardware requirements. . Let&#39;s train a new learner on ResNet50, a 50 layer neural net, using the same training loop that gave us our best results so far. . learn = cnn_learner(dls, resnet50, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(3, 3e-3) learn.unfreeze() learn.fit_one_cycle(6, lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss accuracy time . 0 | 0.772792 | 0.367263 | 0.889716 | 00:21 | . 1 | 0.397766 | 0.222153 | 0.930988 | 00:21 | . 2 | 0.206646 | 0.191509 | 0.940460 | 00:21 | . epoch train_loss valid_loss accuracy time . 0 | 0.135438 | 0.191751 | 0.941137 | 00:29 | . 1 | 0.108808 | 0.187677 | 0.947226 | 00:29 | . 2 | 0.084056 | 0.183770 | 0.945196 | 00:29 | . 3 | 0.065396 | 0.188010 | 0.943843 | 00:29 | . 4 | 0.053252 | 0.184356 | 0.948579 | 00:29 | . 5 | 0.046966 | 0.187739 | 0.946549 | 00:29 | . 94.6% accuracy. We&#39;re getting very close to state of the art performance highlighted at the beginning of this post. . Image Normalisation and Transformation . Until now, we&#39;ve not really touched the data itself. The only transform we&#39;ve made is resizing our images to 224x224 (cropping across the larger dimension). We may be able to get a model to generalise better (and hence perform better on our validation set) by including image normalisation and/or transformations. . Below we initially resize our images 460x460, again cropping across the larger dimension. We then apply further image transforms by batch (which can be done on the GPU and hence is faster). We also normalise our images to ImageNet stats, which is the dataset on which our architecture was pre-trained. . aug_transforms is a utility function that easily creates a list of transforms for us. By default, these include flip (horizontal, not vertical), rotate, zoom, warp and lighting transforms. We additionally add min_scale which tells aug_transforms to take a RandomResizedCrop. . Let&#39;s see if it helps. . Note: Batch transforms can only be performed on images of the same dimensions, which is why we use item transforms initially to resize the images. . petsdata = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(500), batch_tfms=[*aug_transforms(size=224, min_scale=0.9), Normalize.from_stats(*imagenet_stats)]) dls = petsdata.dataloaders(path/&#39;images&#39;) . learn = cnn_learner(dls, resnet50, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(3, 3e-3) learn.unfreeze() learn.fit_one_cycle(6, lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss accuracy time . 0 | 0.783364 | 0.348944 | 0.892422 | 00:25 | . 1 | 0.429991 | 0.242883 | 0.920839 | 00:25 | . 2 | 0.244330 | 0.196868 | 0.935724 | 00:25 | . epoch train_loss valid_loss accuracy time . 0 | 0.167587 | 0.183922 | 0.941813 | 00:33 | . 1 | 0.167674 | 0.192896 | 0.943843 | 00:33 | . 2 | 0.136622 | 0.185411 | 0.943166 | 00:33 | . 3 | 0.117509 | 0.183046 | 0.943843 | 00:33 | . 4 | 0.115614 | 0.181052 | 0.946549 | 00:33 | . 5 | 0.092702 | 0.181630 | 0.946549 | 00:33 | . No real improvement here, but we should bear in the mind the quality of our data is reasonably high. When in production this model may have to deal with less perfect images, such as camera phone images at strange angles. Our transforms will almost certainly have helped the model&#39;s performance in these circumstances. . Progressive Scaling . An interesting discovery recently is that its sometimes possible to train on lower resolution images to begin with, capturing those weights, and then re-train on more typically sized images (eg. 224x224) and achieve better accuracy. . We tried this with our pets data set across hundreds of different combinations and were unable to get any noticeable improvement, in many cases it made it worse. We expect this is because our model is already performing so well, and a lot of the errors are those that humans would also make. . Half Precision Floating Point . Appending .to_fp16() to our learnerenables mixed precision training, which means we use half precision floating point numbers where possible during training. . This allows models to train faster and requires less GPU memory. The lack of precision doesn&#39;t appear to hurt learning in most cases and can actually improve accuracy and generalisation (its in effect a form of regularisation. . Let&#39;s try it on our previous model. . petsdata = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(500), batch_tfms=[*aug_transforms(size=224, min_scale=0.9), Normalize.from_stats(*imagenet_stats)]) dls = petsdata.dataloaders(path/&#39;images&#39;) learn = cnn_learner(dls, resnet50, loss_func=F.cross_entropy, metrics=accuracy).to_fp16() learn.fit_one_cycle(3, 3e-3) learn.unfreeze() learn.fit_one_cycle(6, lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss accuracy time . 0 | 0.895464 | 0.385806 | 0.891746 | 00:18 | . 1 | 0.444253 | 0.284117 | 0.920162 | 00:18 | . 2 | 0.246553 | 0.221550 | 0.933018 | 00:19 | . epoch train_loss valid_loss accuracy time . 0 | 0.190303 | 0.213008 | 0.939784 | 00:23 | . 1 | 0.175834 | 0.207476 | 0.935724 | 00:23 | . 2 | 0.143089 | 0.201775 | 0.941813 | 00:23 | . 3 | 0.115001 | 0.201299 | 0.935047 | 00:23 | . 4 | 0.099792 | 0.193420 | 0.945873 | 00:23 | . 5 | 0.099431 | 0.194508 | 0.945196 | 00:23 | . This really worked out for us. Almost identical accuracy, but a 30-50% performance bump. This ties in nicely to our final technique, epochs. . Epochs . Clearly any of the models we&#39;ve trained above could have benefitted from additional epochs. The reason we leave this for the end is that when prototyping a model, we want to get to the best possible starting point fast. Only once we have a good feel for what is working and what isn&#39;t do we want to try extend the training duration. . Like most parts of Deep Learning, this is definitely more an art than a science. Often it requires multiple iterations, especially when using the One Cycle training process where epoch by epoch results are not always comparable. . Let&#39;s take our above model but train for 20 epochs during the unfrozen training. . learn = cnn_learner(dls, resnet50, loss_func=F.cross_entropy, metrics=accuracy).to_fp16() learn.fit_one_cycle(3, 3e-3) learn.unfreeze() learn.fit_one_cycle(20, lr_max=slice(1e-6,1e-4)) . epoch train_loss valid_loss accuracy time . 0 | 0.846609 | 0.348242 | 0.893775 | 00:19 | . 1 | 0.427032 | 0.260666 | 0.922192 | 00:19 | . 2 | 0.245723 | 0.221145 | 0.933018 | 00:19 | . epoch train_loss valid_loss accuracy time . 0 | 0.170690 | 0.213212 | 0.935724 | 00:23 | . 1 | 0.163887 | 0.208536 | 0.936401 | 00:23 | . 2 | 0.163992 | 0.202402 | 0.933694 | 00:23 | . 3 | 0.143891 | 0.187923 | 0.943843 | 00:23 | . 4 | 0.127290 | 0.194159 | 0.945196 | 00:23 | . 5 | 0.107794 | 0.195019 | 0.940460 | 00:23 | . 6 | 0.094830 | 0.190134 | 0.946549 | 00:23 | . 7 | 0.085796 | 0.194062 | 0.941813 | 00:23 | . 8 | 0.071700 | 0.189948 | 0.944520 | 00:23 | . 9 | 0.062630 | 0.184896 | 0.947226 | 00:23 | . 10 | 0.053348 | 0.188872 | 0.943843 | 00:23 | . 11 | 0.052165 | 0.183817 | 0.947903 | 00:23 | . 12 | 0.048918 | 0.190244 | 0.947226 | 00:23 | . 13 | 0.046626 | 0.193961 | 0.944520 | 00:23 | . 14 | 0.048212 | 0.193595 | 0.943843 | 00:23 | . 15 | 0.040163 | 0.191212 | 0.944520 | 00:23 | . 16 | 0.036564 | 0.187705 | 0.947226 | 00:23 | . 17 | 0.037002 | 0.196278 | 0.946549 | 00:23 | . 18 | 0.034599 | 0.188538 | 0.945873 | 00:23 | . 19 | 0.039673 | 0.193827 | 0.945873 | 00:23 | . In this case, we can see our training loss improves sharply, but it&#39;s the accuracy we care about, and these additional epochs do not translate into a better model. . Putting It Altogether . As a final experiment, let&#39;s try training an even deeper architecture across a larger number of epochs, using all the additional tweaks we&#39;ve already run through. . petsdata = DataBlock(blocks = (ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(seed=42), get_y=using_attr(RegexLabeller(r&#39;(.+)_ d+.jpg$&#39;), &#39;name&#39;), item_tfms=Resize(460), batch_tfms=[*aug_transforms(size=224, min_scale=0.75), Normalize.from_stats(*imagenet_stats)]) dls = petsdata.dataloaders(path/&#39;images&#39;) learn = cnn_learner(dls, resnet152, loss_func=F.cross_entropy, metrics=accuracy).to_fp16() . learn.lr_find() . SuggestedLRs(lr_min=0.004786301031708717, lr_steep=0.0008317637839354575) . learn.fit_one_cycle(3, lr_max=4e-3) . epoch train_loss valid_loss accuracy time . 0 | 0.782812 | 0.477461 | 0.861976 | 00:34 | . 1 | 0.446152 | 0.255121 | 0.926252 | 00:34 | . 2 | 0.251272 | 0.212952 | 0.935047 | 00:35 | . learn.unfreeze() learn.lr_find() . SuggestedLRs(lr_min=6.309573450380412e-08, lr_steep=9.999999747378752e-06) . learn.fit_one_cycle(20, lr_max=slice(1e-7,1e-5)) . epoch train_loss valid_loss accuracy time . 0 | 0.174611 | 0.209708 | 0.941137 | 00:44 | . 1 | 0.164954 | 0.203595 | 0.941813 | 00:44 | . 2 | 0.163552 | 0.203326 | 0.940460 | 00:45 | . 3 | 0.156529 | 0.191764 | 0.945196 | 00:44 | . 4 | 0.152680 | 0.192026 | 0.943843 | 00:44 | . 5 | 0.151489 | 0.184538 | 0.943843 | 00:45 | . 6 | 0.139727 | 0.184647 | 0.947903 | 00:45 | . 7 | 0.132308 | 0.187418 | 0.945196 | 00:45 | . 8 | 0.134661 | 0.184532 | 0.946549 | 00:45 | . 9 | 0.123487 | 0.181255 | 0.947903 | 00:45 | . 10 | 0.119384 | 0.184402 | 0.948579 | 00:44 | . 11 | 0.117015 | 0.182416 | 0.949256 | 00:45 | . 12 | 0.118330 | 0.180863 | 0.947903 | 00:45 | . 13 | 0.120594 | 0.178748 | 0.949256 | 00:45 | . 14 | 0.116538 | 0.180287 | 0.947903 | 00:44 | . 15 | 0.116311 | 0.177829 | 0.951962 | 00:45 | . 16 | 0.125755 | 0.182109 | 0.947903 | 00:45 | . 17 | 0.113480 | 0.182799 | 0.951286 | 00:45 | . 18 | 0.107505 | 0.179559 | 0.948579 | 00:45 | . 19 | 0.110869 | 0.179365 | 0.950609 | 00:44 | . Finally we broke 95% accuracy. That would put us third in the table in our intro, a pretty good result for such a limited amount of work and training. . I&#39;d love to hear from anyone who managed to get even higher accuracy using fastai. Please reach out. .",
            "url": "https://pdito.github.io/blog/deep%20learning/fastai/fastai2/cat/dog/oxford%20iit%20pet/finetuning/one%20cycle/epoch/fp16/progressive%20scaling/discriminative%20learning%20rates/2020/05/15/oxford-iit-pet-dataset-and-fastai2.html",
            "relUrl": "/deep%20learning/fastai/fastai2/cat/dog/oxford%20iit%20pet/finetuning/one%20cycle/epoch/fp16/progressive%20scaling/discriminative%20learning%20rates/2020/05/15/oxford-iit-pet-dataset-and-fastai2.html",
            "date": " • May 15, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "MNIST 99 Percent",
            "content": "Introduction . In my previous blog post I ran through classification for a subset of the MNIST data (3s and 7s only) as a learning experience, following along with Fastbook chapter 4. . From here, I look to take what I&#39;ve learned previously to build a model to tackle the full MNIST data set, attempting to eventually hit an accuracy of &gt; 99% on my validation set. . I won&#39;t be going into as much detail for each step, so please review the previous blog post for a verbose explanation of what is going on. . Getting Started . As usual, we start by importing the necessary libraries. . from fastai2.vision.all import * from utils import * import itertools import math matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . We then use fastai&#39;s built in untar_data function to download and extract the full MNIST data set. . path = untar_data(URLs.MNIST) path.ls() . (#2) [Path(&#39;/home/pdito/.fastai/data/mnist_png/training&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_png/testing&#39;)] . It looks like we have both training and testing data sub-folders. In this case, we&#39;ll use the testing data as our validation data. . Note: Really we should split our training data into training and validation data and keep our testing data separate, but since we are not building something that will ever make it into production, we use the simpler approach. . Next we iterate through all the images to create a list of all our training and validation images. . train_path = (path/&#39;training&#39;).ls().sorted() valid_path = (path/&#39;testing&#39;).ls().sorted() . train_images = list(itertools.chain.from_iterable(([x.ls().sorted() for x in train_path]))) valid_images = list(itertools.chain.from_iterable(([x.ls().sorted() for x in valid_path]))) . We then convert the list into a tensor, where dimension 0 represents each individual image. . train_x = [tensor(Image.open(o)) for o in train_images] valid_x = [tensor(Image.open(o)) for o in valid_images] . train_x = torch.stack(train_x).float()/255 valid_x = torch.stack(valid_x).float()/255 train_x.shape, valid_x.shape . (torch.Size([60000, 28, 28]), torch.Size([10000, 28, 28])) . As usual, the tensor is then reshaped to combine the row and column pixel images into one long tensor, row by row. . train_x = train_x.view(-1, 28*28) valid_x = valid_x.view(-1, 28*28) train_x.shape . torch.Size([60000, 784]) . Next, we need to create our labels. We can use the same list we used to create our train_x and valid_x tensors, iterating though to generate a tensor of values (in this case an int for the number) based on the parent folder name of the image. . train_y = torch.stack([tensor(int(os.path.basename(os.path.dirname(o)))) for o in train_images]) valid_y = torch.stack([tensor(int(os.path.basename(os.path.dirname(o)))) for o in valid_images]) train_y.shape, valid_y.shape . (torch.Size([60000]), torch.Size([10000])) . We use the zip function to create a list of tuples for the images and their labels. . train_dset = list(zip(train_x, train_y)) valid_dset = list(zip(valid_x, valid_y)) . Now we have our training set we can create our Dataloaders, which pass mini-batches of our data to our training model. Note, its typically good practice to shuffle our training data. In our example, this step is essential. Since if we don&#39;t shuffle, most mini-batches will contain images of only one number (as our data set is ordered by folder). . train_dl = DataLoader(train_dset, batch_size=256, shuffle=True) valid_dl = DataLoader(valid_dset, batch_size=256, shuffle=False) . We then create a function which can be used to randomly initialise our parameters, applying .requires_grad_()to tell PyTorch to calculate our gradients. . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() . Now we create our model. In this case we are starting with a simple linear model, wx + b. We are however applying a log softmax function to the result. Softmax in effect squashes our output vector to values between 0 and 1, where those values sum to 1. Our output vector can be interpreted as the probability of something belonging to a given class. . We also take the log of the results, for reasons which are explained next. . def model(xb): return torch.log_softmax((xb@weights + bias), 1) #Example of log_softmax #def log_softmax(x): # return x - x.exp().sum(-1).log().unsqueeze(-1) . For our loss function, we use Negative Log Likelihood. The better our prediction, the lower the NLL. This function focuses only on our prediction for what would have been the correct class. . As an example, lets assume our dataset only contains numbers 1 - 4. For a given image, our softmax output is [0.1, 0.1, 0.1, 0.7] and our label tensor is [0, 0, 0, 1]. In this case our NLL is the negative log of (0 * 0.1) + (0 * 0.1) + (0 * 0.1) + (1 * 0.7). In other words -ln(0.7) = 0.155. . In that case, our model was making a correct guess with 70% confidence. Let&#39;s now look at the example where that guess was incorrect, by changing our label tensor to [0, 0, 1, 0]. In this case our NLL is the negative log of (0 * 0.1) + (0 * 0.1) + (1 * 0.1) + (0 * 0.7) = -ln(0.1) = 1. So a much higher loss. . The reason we took the log of softmax earlier is because the nll_loss function expects its input to be the log of probabilities as opposed to the probabilities themselves. . def mnist_loss(predictions, targets): return F.nll_loss(predictions, targets) loss_func = mnist_loss #Example of NLL #def mnist_loss_manual(predictions, targets): return -predictions[range(targets.shape[0]), targets].mean() . Next we define our step process, which calculates our predictions for a given mini-batch, calculates the loss of those predictions using our loss function and then calculates the gradients of our parameters based on that loss. . def calc_grad(xb, yb, model): preds = model(xb) loss = loss_func(preds, yb) loss.backward() . We then create our training function, which represents an entire epoch. In this case it loops through every mini_batch, calculating the gradients, adjusting our parameters by their gradient multiplied by the learning rate and then resetting the gradients to zero (since they are additive otherwise, which is not what we want). . def train_epoch(model, lr, params): for xb, yb in train_dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad * lr p.grad.zero_() . Right now we only have a loss to measure performance. This is great for training, but not great for us to know how we&#39;re doing. Below we create a function that outputs the accuracy of a given mini batch (taking the index of our highest probability prediction and comparing that to our label for each image). . We then create a function that performs this on our entire validation set that we can call after each training epoch. . def batch_accuracy(xb, yb): preds = torch.argmax(xb, dim=1) return (preds == yb).float().mean() def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb, yb in valid_dl] return round(torch.stack(accs).mean().item(), 6) . Now it&#39;s time to set our model in to action. We start by initialising our parameters, and we also define a learning rate. . weights = init_params((28*28,10)) bias = init_params(10) params = weights, bias lr = 1. . Press play... . def train_model(model, epochs, lr): for i in range(epochs): train_epoch(model, lr, params) print(validate_epoch(model), end=&#39; &#39;) train_model(model, 20, lr) . 0.853223 0.86416 0.875488 0.880273 0.892285 0.890625 0.894043 0.891699 0.897949 0.900977 0.899805 0.905762 0.906055 0.901367 0.908789 0.906543 0.90957 0.904102 0.900391 0.906934 . Around 90% accuracy. Pretty good for a simple linear model. Our model struggles to improve much beyond the 10th epch, perhaps a learning rate that is too high. . Cleaning Up Code with PyTorch/fastai . Let&#39;s simplify our code by using PyTorch&#39;s built in nn.Linear to create our model. This also handles parameter initialisation for us. . linear_model = nn.Linear(28*28, 10) w, b = linear_model.parameters() lr = 0.1 . Since we are no longer taking the log_softmax in our model, we can introduce the PyTorch loss function F.cross_entropy which combines both log softmax and NLL into one function. . loss_func = F.cross_entropy . To tidy thing up, we can also wrap our step and zero grad functions into an optimiser class. . class BasicOptim: def __init__(self, params, lr): self.params, self.lr = list(params), lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr def zero_grad(self, *args, **kwargs): for p in self.params: p.grad = None opt = BasicOptim(linear_model.parameters(), lr) def train_epoch_lm(model): for xb, yb in train_dl: calc_grad(xb, yb, model) opt.step() opt.zero_grad() def train_model_lm(model, epochs): for i in range(epochs): train_epoch_lm(model) print(validate_epoch(model), end=&#39; &#39;) . train_model_lm(linear_model, 20) . 0.88291 0.895898 0.902441 0.902734 0.909668 0.907422 0.911035 0.90957 0.911621 0.912793 0.912793 0.915137 0.91416 0.914746 0.916406 0.915918 0.918066 0.915918 0.916406 0.918164 . Better results than before, around 92% accuracy. Why, since nothing changed in our actual model architecture? . Actually, something did change, we reduced the learning rate from 1.0 to 0.1. Everything else remain consistent, just represented in a cleaner way using less and more reusable code. . Replacing BasicOptim with SGD . To further simplify, fastai provides us with a built in SGD class, similar to the BasicOptim class we created above. . linear_model = nn.Linear(28*28, 10) opt = SGD(linear_model.parameters(), lr) train_model_lm(linear_model, 20) . 0.883691 0.894336 0.901367 0.903223 0.908203 0.908984 0.910156 0.911523 0.91543 0.912305 0.916895 0.916309 0.914453 0.915723 0.91582 0.918555 0.916309 0.915332 0.917773 0.918164 . Again, 92% accuracy. Similar results, which makes sense, since nothing has changed. . Using fastai Learner . Finally, before we start to improve our model, we use a fastai Learnerto replace out training loop in order to further simplify our code. . dls = DataLoaders(train_dl, valid_dl) learn = Learner(dls, nn.Linear(28*28,10), opt_func=SGD, loss_func=F.cross_entropy, metrics=batch_accuracy) learn.fit(20, lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.521860 | 0.446785 | 0.889100 | 00:01 | . 1 | 0.408741 | 0.378355 | 0.899300 | 00:01 | . 2 | 0.373681 | 0.350100 | 0.906000 | 00:01 | . 3 | 0.355128 | 0.335873 | 0.907800 | 00:01 | . 4 | 0.347222 | 0.323164 | 0.912600 | 00:01 | . 5 | 0.331390 | 0.315915 | 0.914400 | 00:01 | . 6 | 0.320100 | 0.309472 | 0.915100 | 00:01 | . 7 | 0.315955 | 0.305583 | 0.915400 | 00:01 | . 8 | 0.307917 | 0.301444 | 0.916400 | 00:01 | . 9 | 0.318447 | 0.298375 | 0.916900 | 00:01 | . 10 | 0.304005 | 0.296194 | 0.917900 | 00:01 | . 11 | 0.305967 | 0.292694 | 0.918400 | 00:01 | . 12 | 0.305479 | 0.290779 | 0.918000 | 00:01 | . 13 | 0.300242 | 0.289566 | 0.920500 | 00:01 | . 14 | 0.304179 | 0.287598 | 0.919700 | 00:01 | . 15 | 0.301372 | 0.286153 | 0.920600 | 00:01 | . 16 | 0.295946 | 0.285350 | 0.920200 | 00:01 | . 17 | 0.294676 | 0.285005 | 0.920200 | 00:01 | . 18 | 0.293234 | 0.284260 | 0.920200 | 00:01 | . 19 | 0.292301 | 0.282514 | 0.919800 | 00:01 | . 92% accuracy here too, just what we wanted to see since again, nothing has changed. . Adding Non-Linearity . Now we do want things to change. To improve our model, let&#39;s add some non-linearity. We&#39;ll sandwich a ReLU activation function in between two linear layers. . neural_net = nn.Sequential( nn.Linear(28*28, 30), nn.ReLU(), nn.Linear(30,10) ) . learn = Learner(dls, neural_net, opt_func=SGD, loss_func=F.cross_entropy, metrics=batch_accuracy) learn.fit(20, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.480248 | 0.397933 | 0.890400 | 00:01 | . 1 | 0.339421 | 0.317796 | 0.910300 | 00:01 | . 2 | 0.307536 | 0.290654 | 0.917800 | 00:01 | . 3 | 0.281915 | 0.269593 | 0.922500 | 00:01 | . 4 | 0.263428 | 0.251829 | 0.927800 | 00:01 | . 5 | 0.247231 | 0.242583 | 0.930700 | 00:01 | . 6 | 0.223604 | 0.224113 | 0.935700 | 00:01 | . 7 | 0.221571 | 0.218439 | 0.936400 | 00:01 | . 8 | 0.209643 | 0.217177 | 0.936700 | 00:01 | . 9 | 0.196226 | 0.199739 | 0.942700 | 00:01 | . 10 | 0.194212 | 0.195394 | 0.943300 | 00:01 | . 11 | 0.189708 | 0.186436 | 0.947200 | 00:01 | . 12 | 0.183434 | 0.182419 | 0.947000 | 00:01 | . 13 | 0.175635 | 0.173868 | 0.949500 | 00:01 | . 14 | 0.166307 | 0.172731 | 0.949800 | 00:01 | . 15 | 0.159394 | 0.169198 | 0.950900 | 00:01 | . 16 | 0.160116 | 0.163204 | 0.952200 | 00:01 | . 17 | 0.151481 | 0.160232 | 0.952000 | 00:01 | . 18 | 0.154878 | 0.161813 | 0.953000 | 00:01 | . 19 | 0.146765 | 0.154272 | 0.955700 | 00:01 | . A pretty significant improvement, over 95% accurate, still using just a very simple architecture. Looking at the output we could definitely afford to train this over more epochs and expect continued improvement. . Using ResNet18 . Since we want to achieve an accuracy of over 99%, let&#39;s use a more complex neural net, in this case, the infamous ResNet18 architecture. . We want to use fastai&#39;s convenience methods for this, so we use theDataBlock function to ensure our data is presented in the desired format. . We have two blocks, an ImageBlock (our data) and a CategoryBlock (our labels). We use PILImage even though our images are greyscale (which would be PILImageBW) as ResNet18 was designed to be used on RGB images and expects its inputs to be structured accordingly. . get_image_files is a helper function that returns all the images under the path. . GrandparenterSplitter let&#39;s us specify the training and validation data split by the images&#39; parent&#39;s parent (ie. grandparent) folder. . parent_labellet&#39;s us define our image labels as the folder name they are contained within. . We then run dataloaderson our DataBlockto get our DataLoaders. . *Note: nothing actually runs in the DataBlock until we call its dataloaders property against a path. . mnist = DataBlock(blocks=(ImageBlock(cls=PILImage), CategoryBlock), get_items=get_image_files, splitter=GrandparentSplitter(train_name=&#39;training&#39;, valid_name=&#39;testing&#39;), get_y=parent_label) dls = mnist.dataloaders(untar_data(URLs.MNIST), batch_sz=128) . We create our Learner, using the resnet18 architecture without pretrained weights. We also directly reference F.cross_entropyin our Learner and use the fastai&#39;s built in accuracy metric. We use fastai&#39;s .fit_one_cycle training method which is a more sophisticated version of .fit. . I&#39;m sure we&#39;ll blog about this soon, but you can read more here. . learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(5) . epoch train_loss valid_loss accuracy time . 0 | 0.189028 | 0.167352 | 0.946300 | 00:27 | . 1 | 0.099857 | 0.090754 | 0.975600 | 00:28 | . 2 | 0.052010 | 0.045006 | 0.987300 | 00:28 | . 3 | 0.021180 | 0.017461 | 0.994700 | 00:28 | . 4 | 0.013487 | 0.015505 | 0.994900 | 00:28 | . Finally, success. 99.5% accuracy after just 5 epochs and two and half minutes of training. We achieve this result in just 4 lines of code. A good indication of the power of fastai. . Doing Something Ridiculous Like Using ResNet152 . Just as an aside, let&#39;s try an extremely deep model to see if we get any improvement. . learn = cnn_learner(dls, resnet152, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(5) . epoch train_loss valid_loss accuracy time . 0 | 0.551158 | 0.449306 | 0.858300 | 02:03 | . 1 | 0.264751 | 0.383378 | 0.880300 | 02:03 | . 2 | 0.075151 | 0.046404 | 0.985700 | 01:58 | . 3 | 0.063741 | 0.031806 | 0.990700 | 02:05 | . 4 | 0.015324 | 0.021084 | 0.994100 | 02:03 | . In this case, no additional benefit from further complexity. . Of course, we could try more epochs, but this comes at risk of overfitting. Investigating what our model got wrong and using that to form the basis of our next steps would be the best way forward. But for now, we&#39;re content with our &gt;99%. . Thanks for reading. .",
            "url": "https://pdito.github.io/blog/deep%20learning/fastai/gradient%20descent/sgd/mnist/2020/05/05/mnist-99-percent.html",
            "relUrl": "/deep%20learning/fastai/gradient%20descent/sgd/mnist/2020/05/05/mnist-99-percent.html",
            "date": " • May 5, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "MNIST from First Principles",
            "content": "Introduction . Whilst the first 3 chapters of Fastbook cover high level overviews of building a model, productionising a model and ethical considerations of data science, from chapter 4 the book starts to take a more in-depth approach to Deep Learning. . In order to improve my understanding, I decided to rewrite the practical parts of chapters from my own perspective, as I learn. My hope is that these posts will remain succinct in general, closely match the book itself, with room for elaboration for the parts I found most taxing. . I will then tackle a different problem using the techniques learned in an additional blog post, before moving on to the following chapter. . Anyone who finds this useful, spots mistakes, has advice, etc. I would love to hear from you. . Getting Started . We&#39;ll start the usual way, importing the necessary libraries. The call to matplotlib.rc, simply tells the library to default its image colormap to greyscale. We do this as we are working with MNIST data which is a greyscale image set. It will save us specifying that fact every time we want to visualise our data later using imshow(). . from fastai2.vision.all import * from utils import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . The focus of this example is to classify any image as either a 3 or 7. We therefore download fastai&#39;s sample of the MNIST data set, which includes numbers from only those categories, represented as 28x28 pixel images. . path = untar_data(URLs.MNIST_SAMPLE) path.ls() . (#3) [Path(&#39;/home/pdito/.fastai/data/mnist_sample/train&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/valid&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/labels.csv&#39;)] . From the above we can see we have a training data folder, train and a validation data folder, valid. Below we see that within these folders we have subfolders for our different classes, 3 and 7. . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/7&#39;)] . Finally, let&#39;s take a look inside one of those class folders to understand how the images are named. We sort the ls output here for consistency. . (path/&#39;train/3&#39;).ls().sorted() . (#6131) [Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10000.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10011.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10031.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10034.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10042.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10052.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/1007.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10074.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10091.png&#39;)...] . Next, let&#39;s create lists to store all the 3s and all the 7s from the training set. . threes = (path/&#39;train/3&#39;).ls().sorted() sevens = (path/&#39;train/7&#39;).ls().sorted() . We can now view the first image from each list, using the Image.open function which is part of the Python Imaging Library, or PIL. . im3 = Image.open(threes[0]) im3 . im7 = Image.open(sevens[0]) im7 . It&#39;s possible to view these numbers as NumPy arrays (or Pytorch tensors) instead, which is how we represent them for the purposes of Machine Learning. . In the below case we take our 3 image and convert it into an array. We then display a slice of the array (which I selected via trial an error to give a good representation) which you can see almost maps out the shape of the 3. . Each number here represents a greyscale number, you can think of 0 being black, 255 being white and anything in between representing the ratio (ie. closer to 0 = darker grey). . array(im3)[5:25,9:21] . array([[ 0, 103, 242, 254, 254, 254, 254, 254, 66, 0, 0, 0], [ 0, 18, 232, 254, 254, 254, 254, 254, 238, 70, 0, 0], [ 0, 0, 104, 244, 254, 224, 254, 254, 254, 141, 0, 0], [ 0, 0, 0, 207, 254, 210, 254, 254, 254, 34, 0, 0], [ 0, 0, 0, 84, 206, 254, 254, 254, 254, 41, 0, 0], [ 0, 0, 0, 0, 24, 209, 254, 254, 254, 171, 0, 0], [ 0, 0, 0, 91, 137, 253, 254, 254, 254, 112, 0, 0], [ 0, 40, 214, 250, 254, 254, 254, 254, 254, 34, 0, 0], [ 0, 81, 247, 254, 254, 254, 254, 254, 254, 146, 0, 0], [ 0, 0, 110, 246, 254, 254, 254, 254, 254, 171, 0, 0], [ 0, 0, 0, 73, 89, 89, 93, 240, 254, 171, 0, 0], [ 0, 0, 0, 0, 0, 0, 1, 128, 254, 219, 31, 0], [ 0, 0, 0, 0, 0, 0, 7, 254, 254, 214, 28, 0], [ 0, 0, 0, 0, 0, 0, 138, 254, 254, 116, 0, 0], [ 0, 0, 0, 0, 0, 25, 240, 254, 254, 34, 0, 0], [ 63, 36, 0, 51, 89, 206, 254, 254, 139, 8, 0, 0], [254, 222, 180, 241, 254, 254, 253, 213, 11, 0, 0, 0], [254, 254, 254, 254, 254, 254, 236, 0, 0, 0, 0, 0], [117, 117, 165, 254, 254, 239, 50, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8) . We can do the same thing for a PyTorch Tensor. . tensor(im3)[5:25,9:21] . tensor([[ 0, 103, 242, 254, 254, 254, 254, 254, 66, 0, 0, 0], [ 0, 18, 232, 254, 254, 254, 254, 254, 238, 70, 0, 0], [ 0, 0, 104, 244, 254, 224, 254, 254, 254, 141, 0, 0], [ 0, 0, 0, 207, 254, 210, 254, 254, 254, 34, 0, 0], [ 0, 0, 0, 84, 206, 254, 254, 254, 254, 41, 0, 0], [ 0, 0, 0, 0, 24, 209, 254, 254, 254, 171, 0, 0], [ 0, 0, 0, 91, 137, 253, 254, 254, 254, 112, 0, 0], [ 0, 40, 214, 250, 254, 254, 254, 254, 254, 34, 0, 0], [ 0, 81, 247, 254, 254, 254, 254, 254, 254, 146, 0, 0], [ 0, 0, 110, 246, 254, 254, 254, 254, 254, 171, 0, 0], [ 0, 0, 0, 73, 89, 89, 93, 240, 254, 171, 0, 0], [ 0, 0, 0, 0, 0, 0, 1, 128, 254, 219, 31, 0], [ 0, 0, 0, 0, 0, 0, 7, 254, 254, 214, 28, 0], [ 0, 0, 0, 0, 0, 0, 138, 254, 254, 116, 0, 0], [ 0, 0, 0, 0, 0, 25, 240, 254, 254, 34, 0, 0], [ 63, 36, 0, 51, 89, 206, 254, 254, 139, 8, 0, 0], [254, 222, 180, 241, 254, 254, 253, 213, 11, 0, 0, 0], [254, 254, 254, 254, 254, 254, 236, 0, 0, 0, 0, 0], [117, 117, 165, 254, 254, 239, 50, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8) . Using Pandas Dataframe, we can even colour code the background to even further illustrate this process. This time without taking a slice, so we can visualise the full image. . df = pd.DataFrame(tensor(im3)) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 42 | 118 | 219 | 166 | 118 | 118 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 103 | 242 | 254 | 254 | 254 | 254 | 254 | 66 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 18 | 232 | 254 | 254 | 254 | 254 | 254 | 238 | 70 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 104 | 244 | 254 | 224 | 254 | 254 | 254 | 141 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 207 | 254 | 210 | 254 | 254 | 254 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 84 | 206 | 254 | 254 | 254 | 254 | 41 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 24 | 209 | 254 | 254 | 254 | 171 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 11 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 91 | 137 | 253 | 254 | 254 | 254 | 112 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 12 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 40 | 214 | 250 | 254 | 254 | 254 | 254 | 254 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 13 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 81 | 247 | 254 | 254 | 254 | 254 | 254 | 254 | 146 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 14 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 110 | 246 | 254 | 254 | 254 | 254 | 254 | 171 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 15 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 73 | 89 | 89 | 93 | 240 | 254 | 171 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 16 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 128 | 254 | 219 | 31 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 17 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 254 | 254 | 214 | 28 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 18 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 138 | 254 | 254 | 116 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 19 0 | 0 | 0 | 0 | 0 | 0 | 19 | 177 | 90 | 0 | 0 | 0 | 0 | 0 | 25 | 240 | 254 | 254 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 20 0 | 0 | 0 | 0 | 0 | 0 | 164 | 254 | 215 | 63 | 36 | 0 | 51 | 89 | 206 | 254 | 254 | 139 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 21 0 | 0 | 0 | 0 | 0 | 0 | 57 | 197 | 254 | 254 | 222 | 180 | 241 | 254 | 254 | 253 | 213 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 22 0 | 0 | 0 | 0 | 0 | 0 | 0 | 140 | 105 | 254 | 254 | 254 | 254 | 254 | 254 | 236 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 23 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 117 | 117 | 165 | 254 | 254 | 239 | 50 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 24 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 25 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 26 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 27 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . Model 1: Pixel Similarity . For our first attempt at categorisation, we&#39;ll use pixel similarity. In this case we find the average pixel value across all the 3s, and then all the 7s, and we compare our image to that to see which it is most similar too. . To begin with, we need to create tensors to hold our images. First, we&#39;ll create a list of tensors for each image. To make sure it works, we print out the resulting length of each list. . three_tensors = [tensor(Image.open(o)) for o in threes] seven_tensors = [tensor(Image.open(o)) for o in sevens] len(three_tensors),len(seven_tensors) . (6131, 6265) . So far, all looks good. We recall earlier from (path/&#39;train/3&#39;).ls().sorted() that there are 6131 files in the 3 folder. . Let&#39;s view an image to confirm, using fastai&#39;s show_image function (since we now have tensors and not images). . show_image(three_tensors[0]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb0e94fe110&gt; . Next we need to convert our lists of images into rank-3 tensors. In other words, the number of items in the list becomes another dimension of the vector. . Note: rank is the number of axes or dimensions in a tensor; shape is the size of each axis of a tensor. . For our 3s for example, we go from a list of 6131, 28x28 tensors, to a 6131x28x28 rank-3 tensor. PyTorch has a built in function called stack to do this. . Given we need to calculate a mean, we need to cast our integer values to floats, so we do this using the float casting method. We also divide our integers by 255, since in Machine Learning pixel values are expected to be between 0 and 1. . Finally, we call .shape on our tensor to make sure the dimensions are as expected. . stacked_threes = torch.stack(three_tensors).float()/255 stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes.shape . torch.Size([6131, 28, 28]) . And to get the rank of the tensor we use: . len(stacked_threes.shape) . 3 . or . stacked_threes.ndim . 3 . To calculate the average across our tensors, we can use the mean function. From the .shape output above we can see the 0 dimension of our tensor represents the number of images, so it is across this axis we wish to calculate the mean. . We can also handily visualise the results to see if what we are doing makes sense. . mean3 = stacked_threes.mean(0) mean7 = stacked_sevens.mean(0) show_image(mean3), show_image(mean7) . (&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb0e94539d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb0e9404dd0&gt;) . Pretty cool! Dark points represent areas where the images generally agree it should be dark, whilst the blurry grey areas represent disagreement between the images. . Now we have our average representations, we can compare our individual image matrix representations to them. . Whilst initially it might seem as simple as taking one away from the other, we have to remember that some differences will result in positive numbers (black areas minus white), whilst others will be negative (white areas minus black) - and these summed would cancel each other out. . Instead, to accurately quantify the difference, we can use the absolute value of the difference (known as the L1 norm) or alternatively the square of the difference square rooted (known as the L2 norm). Both produce a positive error number. . Let&#39;s calculate both for our first 3 that we&#39;ve already spent some time with. . our_3 = stacked_threes[0] dist_3_abs = (our_3 - mean3).abs().mean() dist_3_sqr = ((our_3 - mean3)**2).mean().sqrt() dist_7_abs = (our_3 - mean7).abs().mean() dist_7_sqr = ((our_3 - mean7)**2).mean().sqrt() dist_3_abs, dist_3_sqr, dist_7_abs, dist_7_sqr . (tensor(0.1074), tensor(0.1912), tensor(0.1441), tensor(0.2780)) . In both cases, the distance between our 3 and our ideal 3 is less than the distance between our 3 and our ideal 7. So already, our model appears to give a correct prediction (albeit in a 50/50). Let&#39;s test on our first 7 as well. . our_7 = stacked_sevens[0] dist_3_abs = (our_7 - mean3).abs().mean() dist_3_sqr = ((our_7 - mean3)**2).mean().sqrt() dist_7_abs = (our_7 - mean7).abs().mean() dist_7_sqr = ((our_7 - mean7)**2).mean().sqrt() dist_3_abs, dist_3_sqr, dist_7_abs, dist_7_sqr . (tensor(0.1716), tensor(0.3031), tensor(0.1095), tensor(0.2196)) . In this case the distance between our 7 and our ideal 3 is greater than the distance betwen our 7 and our ideal 7. Another correct prediction. . Note: PyTorch already provides these loss functions in torch.nn.functional which are slightly cleaner to use as illustrated below. We can see the values match. . F.l1_loss(our_7, mean3), F.mse_loss(our_7,mean3).sqrt() . (tensor(0.1716), tensor(0.3031)) . Now our initial sense checked has passed, we want to calculate the actual performance on our validation set. Since error alone is not a good way for humans to understand our performance, we instead measure accuracy. That is, what percentage of our validation images did the model categorise correctly. . To begin with, we must get our validation data sets into tensors in the correct format. . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid/3&#39;).ls()]) valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid/7&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape, valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . As we&#39;re going to be calculating the difference between two &#39;numbers&#39; multiple times, we write a function to handle this. . mean((-1,-2)) represents taking the mean over the last and second last dimensions (in our tensors this is the x and y axis of the image). . def mnist_dist(a, b): return (a-b).abs().mean((-1,-2)) mnist_dist(our_7, mean3) . tensor(0.1716) . So our function works for one example, but what happens if we pass it our tensor of validation images. . valid_3_dist = mnist_dist(valid_3_tens, mean3) valid_3_dist, valid_3_dist.shape . (tensor([0.1745, 0.1193, 0.1334, ..., 0.1181, 0.1370, 0.1406]), torch.Size([1010])) . Instead of throwing an error, PyTorch uses something called broadcasting. This is where PyTorch automatically expands the tensor with the smaller rank to have the same size as the one with the larger rank. . In other words, mean3 moves from being a 28x28 tensor to a 1010x28x28 tensor, where each of the 28x28 images are identical for all 1010 values across the 0 dimension. . This process doesn&#39;t take up additional memory, it is simply implied. Also worth noting, this calc is done in C and on the GPU if you are using CUDA. . Stepping through the mnist_dist function for this calculation:- . a-b - uses broadcasting to subtract mean from every image in our validation set. (We now have a 1010x28x28 tensor). | .abs() - takes the absolute value of every element of our tensor. (We now have a 1010x28x28 tensor). | .mean((-1,-2)), takes the mean across all the values in our last and second to last dimension, in this case our height (28) and width (28) dimensions. (We now have a 1010 tensor). | . Knowing all this allows us to write a function to output our results across an entire validation set. Given we only have two classes, we can write a function called is_3, knowing that if it returns false, this is equivalent to it being a 7. . def is_3(x): return mnist_dist(x , mean3) &lt; mnist_dist(x, mean7) . We can test this on our example cases from earlier, the first 3 and first 7 in our training set. . is_3(three_tensors[0]), is_3(seven_tensors[0]) . (tensor(True), tensor(False)) . It works! is_3 returns True for our first 3 and False for our first 7. Using broadcasting, we are able to perform the same calculation on our entire validation set. . Given a Boolean converted to a float is 0 (False) or 1 (True), we can actually take the mean of our resulting tensors to get the accuracy. . accuracy_3s = is_3(valid_3_tens).float().mean() accuracy_7s = (1 - is_3(valid_7_tens).float()).mean() accuracy_3s, accuracy_7s, (accuracy_3s + accuracy_7s) / 2 . (tensor(0.9168), tensor(0.9854), tensor(0.9511)) . Pretty good results for a really basic model. 91.7% accuracy on 3s, 98.5% accuracy on 7s and 95.1% accuracy overall. . This gives us a good benchmark to see if we can take our performance to the next level. . Model 2: Stochastic Gradient Descent . Our above example doesn&#39;t allow for any improvement in performance, outside of expanding the training set. There is no way for it to get better and better by adjusting parameters. In order to take advantage of deep learning, we need to come up with a function which has weights that can be updated. . A sensible approach in this case would be a function that represents each pixel as having a weight, with that weight being higher for those pixels most likely to be white (remember in MNIST our background is black and writing is white) for a particularly category. . For example: . def pr_three(x, w) = (x * w).sum() . In the above example, X represents our image, in this case a vector. Previously we&#39;ve regarded our image as a 28x28 tensor, but if we stack up all the rows in a single line, we end up with a 784 vector (which can still be called a tensor). . Likewise, W represents our weights. We can gradually update these weights, making them a little bit better each time, until they are able to best predict if a given image is a 3 or not. . The process is as follows:- . Initialise the weights. | For each image use these weights to predict whether the image is a 3 or 7. | Based on these predictions, calculate the model&#39;s performance (its loss). | Calculate the gradient, which measures for each weight, how changing that weight would change the loss. | Step (that is, change) all weights based on that calculation. | Go back to step 2, and repeat. | Stop - when you decide to (because the model is good enough or you don&#39;t want to wait any longer, or performance stops improving). | gv(&#39;&#39;&#39; init-&gt;predict-&gt;loss-&gt;gradient-&gt;step-&gt;stop step-&gt;predict[label=repeat] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G init init predict predict init&#45;&gt;predict loss loss predict&#45;&gt;loss gradient gradient loss&#45;&gt;gradient step step gradient&#45;&gt;step step&#45;&gt;predict repeat stop stop step&#45;&gt;stop Let&#39;s get back to our MNIST sample dataset of 3s and 7s. First we combine our 3s and 7s into the same tensor. . train_x = torch.cat([stacked_threes, stacked_sevens]) train_x.shape . torch.Size([12396, 28, 28]) . Next, as explained earlier, we want to flatten our image into a single vector, resulting in a tensor of the form (images, pixel values) where images represents every image and pixel values represents the pixel values in vector form of any given image. . To do this we can use PyTorch&#39;s view method, which changes the shape of a tensor without changing its contents. The -1 is a special parameter that says, &#39;make this axis as big as necessary to fit all the data&#39; rather than setting it to a specific value. . train_x = train_x.view(-1, 28*28) train_x.shape . torch.Size([12396, 784]) . Now we have our image data, but since we have combined 3s and 7s into the same dataset, we need to label those values so our model can calculate the loss. We&#39;ll use a 1 for threes and 0 for 7s. . Because we know we have combined our two datasets, we can simply take the length of threes and make the y values 1s, the length of sevens and make them 0s, and then combine. These will match up with our train_x tensor. . train_y = tensor([1]*len(threes) + [0]*len(sevens)) train_y.shape . torch.Size([12396]) . For convention, we need to convert this vector into a matrix, in this case of 12396 rows and 1 column. To do this we can use the unsqueeze method, and specify the position 1 dimension (remember these are zero base). . train_y = train_y.unsqueeze(1) train_y.shape . torch.Size([12396, 1]) . Now we have our data structured in the way we want, we need to combine it into a Dataset that PyTorch can use. It expects to be able to index in to a DataSet and get a tuple of (x, y) returned. We can combine the zip and list Python functions to achieve this. . dset = list(zip(train_x, train_y)) x, y = dset[0] x.shape, y.shape . (torch.Size([784]), torch.Size([1])) . Next we do the same for our validation data. . valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1] * len(valid_3_tens) + [0] * len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x, valid_y)) x, y = valid_dset[0] x.shape, y.shape . (torch.Size([784]), torch.Size([1])) . Now we need to create our weights tensor. The best way to do this is with an initialisation function, since we want our initial parameters to be random and we don&#39;t want to have to repeat that process every time we use this model. . We also add the requiresgrad() method to tell PyTorch we want to track the gradients of the parameters in this tensor. We&#39;ll touch on this in more detail later. . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() weights = init_params((28*28, 1)) . A problem with our proposed function of x * w is that where x (ie the pixel value) is zero, then x * w will be zero. The gradient will be zero, and the model won&#39;t be able to improve. To overcome this problem, we add a bias unit. (Remember from school, the straight line equation of y = wx + b. . bias = init_params(1) . Note, in neural networks, w = weights, b = bias and together these make up the parameters. . Let&#39;s go ahead and calculate the loss for one image. Note, .T simply transposes the weights, which is necessary to perform the multiplication. . (train_x[0]*weights.T).sum()+bias . tensor([10.2040], grad_fn=&lt;AddBackward0&gt;) . Clearly we have the option here to loop through every image and sum the results to calculate our overall loss, but this would be very slow. Instead, we use matrix multiplication (view here if unsure how it works), which calculates w * x for every row of the matrix. This is the fundamental mathematical operation that powers all of deep learning. . In Python, matrix multiplication is represented by the @ operator. We create the function below to perform this for an entire DataSet. Note:- xb refers to x-batch, ie. a mini batch of our training data. No need to worry about what that is for now. . def linear1(xb): return xb@weights + bias preds = linear1(train_x) preds . tensor([[10.2040], [ 1.3076], [ 6.9170], ..., [ 8.5185], [ 4.9300], [ 0.4510]], grad_fn=&lt;AddBackward0&gt;) . To check our accuracy, lets see if our preds results are greater than zero. This confused me at first, but if you look how we initialise our parameters, we use a normal distribution with a standard deviation of 1. So it seems likely that around half of our predictions will be positive and half will be negative. Therefore we pick 0.0 for training efficiency purposes. . It&#39;s worth noting that we could have picked any number for the threshold, but picking 0.0 saves our model from having to &#39;drag&#39; its values away from where they were initialised to having a mean around a different threshold. The end result and accuracy would be same, it may (although not necessarily) just take a little longer to get there. . corrects = (preds&gt;0.0).float() == train_y corrects . tensor([[ True], [ True], [ True], ..., [False], [False], [False]]) . corrects.float().mean().item() . 0.4757179617881775 . Given we are using a normal distribution with a standard deviation of 1.0 to initialise our weights, it makes sense our model is around 50% accurate (since it has a 50/50 chance of being so). . So what happens when we adjust one of our weights: . weights[0] *= 1.0001 preds = linear1(train_x) ((preds &gt; 0.0).float() == train_y).float().mean().item() . 0.4757179617881775 . It looks like our weight&#39;s gradient is 0 with respect to accuracy. There is a simple explanation for this. Changing the weight for 1 pixel in the image by a tiny amount, will, in most cases, fail to change our model&#39;s final prediction. Accuracy is a direct function of this prediction. . So instead, we need a measure of performance that will respond better to minor changes in pixel weightings. We call this a loss function. . Since we know our y values are either 1 or 0, we can simply measure the error as the difference between expected result and prediction. In other words, if our prediction was 0.8, and the y value was 1 (it was 3!) then our loss would be 0.2. If our prediction was 0.8 and the y value was 0 (it was a 7) then our loss would be 0.8. . For this to work, we need to ensure our predictions always lie between 0 and 1. To do that, we can use the sigmoid function. . plot_function(torch.sigmoid, min=-5, max=5) . So let&#39;s create our loss function. We use torch.where which is basically a far more efficient CUDA speed list comprehension. In this case our function says if the prediction should be 1, then calculate the loss as 1 - prediction, otherwise set the loss to our prediction, given the only other value is 0 (and so loss would be prediction - 0). . Before passing our values into this function, we run .sigmoid() to make sure all our predictions fall between 1 and 0. . def mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1 - predictions, predictions).mean() . In simple terms, once we introduce this loss function, our model should learn that a higher prediction corressponds to more confidence an image is a 3. . Now we have our data and loss function, we need to train our model. Instead of doing this image by image and calculating the gradients each time, a better approach is to use a mini-batch of images, calculating the average loss for those as a group, and the subsequent gradients. This allows our model to learn through an epoch, not only at the end of an entire epoch. . To pass these batches we use a PyTorch class called a DataLoader. Remember, we already created our Dataset, dset and Validation Dataset, valid_dset earlier. We can pass these into DataLoader with a desired batch_size to create our DataLoaders. . dl = DataLoader(dset, batch_size=256) valid_dl = DataLoader(valid_dset, batch_size=256) . Next let&#39;s create a function to calculate our gradients at each step. We pass the function a mini-batch of training data (xb) and labels (yb) along with our model, calculate our predictions, calculate our subsequent loss and then .backward() calculates the gradients on our parameters with respect to that loss function. . The idea is that whenever we call .backward() on a function, it computes the gradient with respect to that function for every parameter which has requires_grad=True. The key word here is EVERY. At first I was wondering how we linked our loss function to our parameters. The answer is, we don&#39;t need to, because its done for everything automatically. . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . Now we&#39;re in a position to create our basic training loop. Training a model often takes many epochs. An epoch is simply one full pass through the model of all your training data. Too many epochs and you&#39;re likely to overfit your training data, so its important to keep an eye on the direction of your validation metric. . Here the function loops through every mini-batch in our training set, calculating the gradients on our parameters at each step and then adjusting the parameters by those gradients multiplied by a pre-specified learning rate. . Note: once we&#39;ve adjusted our parameters, we zero out our gradients since .backward() adds the gradients to any that are already stored, which is not what we want to do across mini-batches. . def train_epoch(model, lr, params): for xb, yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad * lr p.grad.zero_() . Finally we need functions to calculate our accuracy, and perform this calculation on our validation set after each epoch. . Our batch_accuracy function takes a batch from our validation data and returns its accuracy, whilst validate_epoch uses this function when looping through ALL our valid_dl Dataset to calculate an average of accuracy over all our mini-batches. . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds &gt; 0.5) == yb return correct.float().mean() def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb, yb in valid_dl] return round(torch.stack(accs).mean().item(), 6) . Now lets re-initialise our parameters. . weights = init_params((28*28,1)) bias = init_params(1) params = weights, bias . And set our model into action... . epochs_to_train = 20 lr = 1. for i in range(epochs_to_train): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.774255 0.898747 0.93486 0.951462 0.956344 0.960251 0.961227 0.9632 0.963689 0.965153 0.965642 0.966618 0.967595 0.968083 0.96906 0.968083 0.968571 0.969568 0.969568 0.969568 . Good progress, 97% accurancy. we&#39;re beating our original model after training for just 20 epochs, which took no time at all. . Model 3: Stochastic Gradient Descent (Again) - with PyTorch. . The above breakdown was really there to aid understanding, the code can be simplified considerably using PyTorch&#39;s built in functionality. . The nn.Linear function replaces both our init_params ands linear functions above. . linear_model = nn.Linear(28*28, 1) . Our weights and bias are contained within a single class as shown below:- . w, b = linear_model.parameters() w.shape, b.shape . (torch.Size([1, 784]), torch.Size([1])) . We can create then an Optimiser class to use this model to train. . class BasicOptim: def __init__(self, params, lr): self.params, self.lr = list(params), lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr def zero_grad(self, *args, **kwargs): for p in self.params: p.grad = None . We initialise our Optimiser as follows. . opt = BasicOptim(linear_model.parameters(), lr) . Then we can once again create our training function. . def train_epoch(model): for xb, yb in dl: calc_grad(xb, yb, model) opt.step() opt.zero_grad() . And our training loop. . def train_model(model, epochs): for i in range(epochs): train_epoch(model) print(validate_epoch(model), end=&#39; &#39;) . And we run 20 epochs for this model... . train_model(linear_model, 20) . 0.4932 0.7627 0.8554 0.9155 0.9345 0.9477 0.956 0.9633 0.9658 0.9672 0.9697 0.9716 0.9736 0.9746 0.9755 0.9765 0.9775 0.9775 0.978 0.9785 . We get very similar results here again, 98% accuracy. . &#160;Model 4 - Stochastic Gradient Descent (Again, Again) - with fastai v2 . fastai provides its own SGD class which does the same as our BasicOptim class above. . This allows for further simplification. . linear_model = nn.Linear(28*28, 1) opt = SGD(linear_model.parameters(), lr) train_model(linear_model, 20) . 0.493164 0.765605 0.856894 0.918417 0.935995 0.95162 0.956503 0.962363 0.965781 0.967734 0.970663 0.972617 0.97457 0.975058 0.976035 0.976523 0.977499 0.977988 0.978476 0.978476 . Again, the same result. . Model 5 - Stocashtic Gradient Descent - fastai v2 Learner . We can again reduce the code required by using fastai&#39;s Learner class. This reduces the need for our training functions / loop. We must first create DataLoaders which pass fastai batches of our data. We then create our Learner by passing in our Dataloaders, the model, the optimisation function, the loss function and (optionally) our metrics. . dls = DataLoaders(dl, valid_dl) learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) learn.fit(20, lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.636961 | 0.503404 | 0.495584 | 00:00 | . 1 | 0.494130 | 0.196144 | 0.835132 | 00:00 | . 2 | 0.182421 | 0.174930 | 0.843474 | 00:00 | . 3 | 0.080600 | 0.104251 | 0.912169 | 00:00 | . 4 | 0.043039 | 0.076627 | 0.934740 | 00:00 | . 5 | 0.028333 | 0.061650 | 0.947988 | 00:00 | . 6 | 0.022267 | 0.052274 | 0.956330 | 00:00 | . 7 | 0.019558 | 0.045992 | 0.963199 | 00:00 | . 8 | 0.018177 | 0.041555 | 0.965653 | 00:00 | . 9 | 0.017346 | 0.038275 | 0.967125 | 00:00 | . 10 | 0.016761 | 0.035755 | 0.970069 | 00:00 | . 11 | 0.016301 | 0.033752 | 0.972031 | 00:00 | . 12 | 0.015918 | 0.032116 | 0.973503 | 00:00 | . 13 | 0.015591 | 0.030750 | 0.974485 | 00:00 | . 14 | 0.015310 | 0.029594 | 0.975957 | 00:00 | . 15 | 0.015066 | 0.028604 | 0.976448 | 00:00 | . 16 | 0.014853 | 0.027748 | 0.977429 | 00:00 | . 17 | 0.014664 | 0.027003 | 0.977920 | 00:00 | . 18 | 0.014495 | 0.026348 | 0.978410 | 00:00 | . 19 | 0.014341 | 0.025770 | 0.978410 | 00:00 | . Again, less code, same results, around 98% accuracy. . &#160;Model 6 - Stochastic Gradient Descent - Adding a Non-linearity . So far, all our models have used a simple linear classifier. To add complexity to our model, enabling it to perform more detailed tasks, we need can add a non-linearity between two linear classifiers (since two linear classifiers on their own can be simplified into a single linear classifier). It is this basic concept that gives us the foundation of a very simple neural network. . This simple neural network can be summarised as:- . neural_net = xb@w1 + b1 neural_net = neural_net.max(tensor(0.0)) neural_net = neural_net@w2 + b2 . That&#39;s really all there is to it. Lines 1 and 3 represent linear classifiers which we are already familiar with, line 2 represents the ReLU function - a complicated way of saying take the maximum of the value or 0. . So how do we code this model. It can be done in 1 line in PyTorch. nn.Sequential creates a module which calls each of the listed layers in turn. . Note: The 30s in the below code represent the number of output activations for w1 and input activations for w2. These are in effect our features. So the model can construct 30 different features, each representing a different mix of pixels, and pass them as inputs (after running through the ReLU function) to the following layer. The higher this number, the higher the complexity of the model. . neural_net = nn.Sequential( nn.Linear(28*28, 30), nn.ReLU(), nn.Linear(30,1) ) . Let&#39;s see if this helps improve our accuracy. We do train for more epochs here, but use a much lower learning rate. . learn = Learner(dls, neural_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) learn.fit(40, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.349672 | 0.389846 | 0.513248 | 00:00 | . 1 | 0.157197 | 0.239514 | 0.794897 | 00:00 | . 2 | 0.085345 | 0.117664 | 0.914132 | 00:00 | . 3 | 0.054955 | 0.078449 | 0.941119 | 00:00 | . 4 | 0.041037 | 0.060892 | 0.955348 | 00:00 | . 5 | 0.034031 | 0.051127 | 0.963690 | 00:00 | . 6 | 0.030067 | 0.044998 | 0.967125 | 00:00 | . 7 | 0.027523 | 0.040819 | 0.967125 | 00:00 | . 8 | 0.025706 | 0.037790 | 0.968597 | 00:00 | . 9 | 0.024309 | 0.035483 | 0.971050 | 00:00 | . 10 | 0.023182 | 0.033657 | 0.972031 | 00:00 | . 11 | 0.022244 | 0.032163 | 0.973013 | 00:00 | . 12 | 0.021448 | 0.030910 | 0.974975 | 00:00 | . 13 | 0.020758 | 0.029838 | 0.974975 | 00:00 | . 14 | 0.020154 | 0.028907 | 0.974975 | 00:00 | . 15 | 0.019618 | 0.028087 | 0.975957 | 00:00 | . 16 | 0.019139 | 0.027358 | 0.976938 | 00:00 | . 17 | 0.018706 | 0.026706 | 0.977429 | 00:00 | . 18 | 0.018313 | 0.026118 | 0.978410 | 00:00 | . 19 | 0.017954 | 0.025585 | 0.978410 | 00:00 | . 20 | 0.017624 | 0.025099 | 0.978901 | 00:00 | . 21 | 0.017318 | 0.024655 | 0.978901 | 00:00 | . 22 | 0.017034 | 0.024248 | 0.979882 | 00:00 | . 23 | 0.016769 | 0.023873 | 0.979882 | 00:00 | . 24 | 0.016521 | 0.023527 | 0.980373 | 00:00 | . 25 | 0.016287 | 0.023207 | 0.982336 | 00:00 | . 26 | 0.016067 | 0.022910 | 0.982336 | 00:00 | . 27 | 0.015860 | 0.022634 | 0.982336 | 00:00 | . 28 | 0.015663 | 0.022377 | 0.982336 | 00:00 | . 29 | 0.015476 | 0.022137 | 0.982336 | 00:00 | . 30 | 0.015298 | 0.021913 | 0.982336 | 00:00 | . 31 | 0.015129 | 0.021703 | 0.982826 | 00:00 | . 32 | 0.014968 | 0.021506 | 0.982826 | 00:00 | . 33 | 0.014813 | 0.021322 | 0.982826 | 00:00 | . 34 | 0.014665 | 0.021148 | 0.982826 | 00:00 | . 35 | 0.014523 | 0.020984 | 0.982826 | 00:00 | . 36 | 0.014387 | 0.020829 | 0.982826 | 00:00 | . 37 | 0.014256 | 0.020682 | 0.982826 | 00:00 | . 38 | 0.014129 | 0.020543 | 0.982826 | 00:00 | . 39 | 0.014007 | 0.020411 | 0.983317 | 00:00 | . Our accuracy has increased to above 98%! . Model 7 - Stochastic Gradient Descent - ResNet18 . We needn&#39;t stop there. Whilst simplicity is typically preferred, its been proven that networks with a higher number of smaller layers get better than results that those with a lower number of larger layers. This also has an efficiency benefit, with these models being faster to train. . As a final approach, let&#39;s train a model with 18 layers. We are using an architecture called resnet18, and a cross entropy loss function, but we won&#39;t go into detail on those here. The purpose is to see the additional accuracy that can be achieved using these deeper networks. . dls = ImageDataLoaders.from_folder(path) learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(1, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.088288 | 0.008928 | 0.997547 | 00:06 | . 99.75% accuracy after one epoch. Not bad! . Summary . We&#39;ve seen many different applications of the same solution, whilst also upping the complexity towards the end to understand the performance improvements. . The next step is to take what we have learned and apply it to the full MNIST training set, for digits 0 thru 9. .",
            "url": "https://pdito.github.io/blog/deep%20learning/fastai/gradient%20descent/sgd/mnist/2020/04/17/fastai2-ch4-mnist-first-principles.html",
            "relUrl": "/deep%20learning/fastai/gradient%20descent/sgd/mnist/2020/04/17/fastai2-ch4-mnist-first-principles.html",
            "date": " • Apr 17, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "A Pokemon Gen 1 Classifier",
            "content": "The Project . Following completion of week 2 of the fastai 2020 course, here is my first project, a Pokemon Gen 1 Classifier. My target is to achieve 90% accuracy metric. As I write, I have no idea how ambitious that is. . The Data . One thing i&#39;ve learned very quickly about Deep Learning, is that gathering and labelling data is often the biggest challenge. Thankfully, i&#39;ve also learned that the community is incredibly generous in sharing data. I definitely intend to gather my own data soon - I feel that&#39;s something key to get experience in early on, but for my first project I wanted to dive into the coding as soon as possible. . I found an excellent dataset on Kaggle perfectly suited to my problem. So big thanks to Lance Zhang for this. . Steps . I&#39;m trying to keep this model as simple as possible, and hope the below steps will cover everything. . Set up. | Get data and build data model. | Train model. | Test on new data. | Productionise. | 1. Set Up . I&#39;m fortunate to have a local Ubuntu box with a GPU capable of training these kind of models (I&#39;ll post about setting that up soon). But typically it is recommended to use something like Paperspace or Colab when starting out and the below should be applicable to any environment you intend to run Jupyter on. . First step, import the necessary libraries. . from utils import * from fastai2.vision.widgets import * . TIP: This initially failed for me. I had to ensure that utils.py, which sits in the fast.ai nbs (notebooks) directory, was copied to the same folder as the notebook I was working on. . 2. The Data Model . There are many ways to get data into your deep learning environment. Kaggle offers a library, which I&#39;ll attempt to use in future, but for now, what I found most comfortable was to download the data to my local machine, inspect it, get to know it, zip it up and put that zip file on Dropbox. I was then able to share that file, and use wget {link} to import it to my deep learning environment. . One point worth noting, is that when you share a Dropbox link, the end part always defaults to dl=0, you should change this to dl=1 for the wget command to work. I believe this option is only available to paid Dropbox accounts, so please bear that in mind too. . You could also upload the .zip via Jupyter&#39;s upload function. . After this step I manually unzipped the file. fast.ai has some nice convenience methods to download and untar data, but for now I don&#39;t believe the library has native support for zipped data. . Below I set the path for my data. . path = Path(&quot;/home/pdito/AI/datasets/pokemon-gen1/&quot;) path.ls() . (#150) [Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Psyduck&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Bulbasaur&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Poliwhirl&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Raticate&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Victreebel&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Tangela&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Venusaur&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Butterfree&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Cubone&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Gastly&#39;)...] . We can see the data is split into folders for each different Pokemon, but there is no training / validation split currently. . Next I create my Datablock. . I have two blocks, and ImageBlock and a CategoryBlock. The ImageBlock represents by independent variables, whilst the CategoryBlock represents the dependent value, in other words, my y or what i&#39;m trying to predict. . In order to get the items, I set this to get_image_files - which recursively polls a provided path to extract all the images. I&#39;m actually still not sure why we don&#39;t specify the path in the DataBlock, and instead call it later when we initialise the DataLoader. I guess the DataBlock is simply defining the structure and the DataLoader is actually initialising something with real data. . Splitter determines my training / validation set split, i&#39;ve chosen to make this 80% / 20% and set a seed so the validation set remains consistent as I iterate. . I set my Category / dependent variable / y to parent_label, as my images are all in subfolders with the Pokemon name. This is super useful part of the fastai library, and I understand there are lots of different options available. . Finally, I transform all my images using a standard Resize. This makes the images 224x224, which should work well as i&#39;m intending to use transfer learning from ResNet34, which was trained on images of this dimension. . pokemon = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(224)) . Next, from my DataBlock, I create my DataLoaders (train and valid) referencing the root folder of my images that we already specified above. . dls = pokemon.dataloaders(path) . I can now visualise the DataLoaders to make sure everything looks correct. . dls.valid.show_batch(max_n=4, nrows=1) . All looks good, so its time to define our architecture. In this case, i&#39;m using the pre-trained resnet34 architecture as a starting point. I&#39;ll then run fastai&#39;s fine tune function (which is designed specifically for transfer learning) to repurpose the model to my requirements (by adding some additional layers to the head and retraining parts of the models with differing learning rates - we don&#39;t need to get into those details at this stage). . I set the metric to accuracy, which I think is the best, human-intepretable output at this stage. . learn = cnn_learner(dls, resnet34, metrics=accuracy) learn.fine_tune(4) . epoch train_loss valid_loss accuracy time . 0 | 4.545189 | 1.752553 | 0.603968 | 00:13 | . epoch train_loss valid_loss accuracy time . 0 | 1.387421 | 0.610193 | 0.854519 | 00:18 | . 1 | 0.632813 | 0.358847 | 0.911830 | 00:18 | . 2 | 0.244674 | 0.301103 | 0.929464 | 00:18 | . 3 | 0.120268 | 0.288353 | 0.930198 | 00:18 | . Surprisingly, with almost no effort, I&#39;m able to achieve over 93% accuracy on the validation set. This is particularly impressive given the structure of the input data. Images include the official animation images, pixelated gameboy images, plush toy images, hand-drawn images. Furthermore, many of the Pokemon in gen 1 have an extremely close likeness amongst their evolution tree. . I had expected to have to fine tune the model and tidy up the input data to get anywhere near this result. Instead, in less than a minute and half of training, i&#39;ve exceeded my initial target. . This is a great indication of the power of transfer learning. I&#39;m keen to revisit this model in future to see how I can improve the accuracy. . Even though I am happy with the results, it&#39;s interesting to look at the items that the model is most incorrect on (in other words, from the set classifications it got incorrect, those which it had most confidence it got correct). . Often these results are visualised in a confusion matrix, but we have too many classes here (151) so instead we just straight to the top losses. . interp = ClassificationInterpretation.from_learner(learn) . #interp.plot_top_losses(5, nrows=1) interp.plot_top_losses(6, figsize=(15,10)) . Seeing this visualisation helps me understand why the model is making mistakes. . Top Left: This image should be excluded. It shows multiple Pokemon. Top Middle: So close! It was obviously hard for the model to pick up detail in this dimly lit image. Top Right: Mislabelled data! Our model is actually correct. This is Alakazam and not Kadabra. Bottom Left: I feel like our model should have got this one, but admitedly the angle is strange. Bottm Middle: It&#39;s unusual to see Marowak with fire, and the colour palette is far more attuned to Magmar. Bottom Right: You can certainly see why the model would choose Gyarados. At a glance, I would do the same. . Overall, there is nothing here that gives me particular concern. . 4. Quick Test . I use a widget to easily allow me to upload files for testing from my local machine. Here I run a test against an image of Pikachu that is not included in the original data set. . uploader = widgets.FileUpload() uploader . img = PILImage.create(uploader.data[0]) category,pred_idx,probs = learn.predict(img) print(f&quot;Who is this?: {category}. Probability: {probs[pred_idx] * 100:.02f}%.&quot;) img . Who is this?: Pikachu. Probability: 99.96%. . It works! . 5. Productionise . If getting good, labelled data is the hardest part of the problem, i&#39;m told productionising your model comes a close second. . I&#39;m going to build this out into a separate notebook and use Voila (to turn a notebook into an app) and Binder (to publish it on the web). Nethertheless, for completeness I&#39;ll provide all the detail below. . First, let&#39;s export our trained model and confirm that the file exists after doing so. . learn.export() path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . I&#39;m renaming the file in terminal from export.pkl to pokemon.pkl so I understand what it is...going forward it will be refered to as the latter. . Everything below this point will form the basis of our &#39;Web App&#39;. . In Deep Learning, prediction is often refered to as inference. I&#39;m going to load in my trained model for inference below. . learn_inf = load_learner(path/&#39;pokemon.pkl&#39;) . We can check that all seems correct by reviewing our potential output categories. . learn_inf.dls.vocab . (#150) [&#39;Abra&#39;,&#39;Aerodactyl&#39;,&#39;Alakazam&#39;,&#39;Alolan Sandslash&#39;,&#39;Arbok&#39;,&#39;Arcanine&#39;,&#39;Articuno&#39;,&#39;Beedrill&#39;,&#39;Bellsprout&#39;,&#39;Blastoise&#39;...] . Seems good! . First we create a label to guide the user. Next we create an Upload button so our user can submit their image. We then create a Classify button so the user can run the classification. Finally we create placeholders to display the uploaded image and the resulting classification. . After that we add an event handler to the Classify button to perform the necessary actions and populate the output placeholders on button click. . Finally, we place all the UI elements into a vertical box (VBox) to keep the UI looking clean. . user_inst = widgets.Label(&#39;Upload your Pokemon - Gen 1 only please!&#39;) btn_upload = widgets.FileUpload() btn_classify = widgets.Button(description=&#39;Classify&#39;) img_user = widgets.Output() lbl_pred = widgets.Label() def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) img_user.clear_output() with img_user: display(img.to_thumb(128,128)) pred, pred_idx, probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]*100:.02f}%.&#39; btn_classify.on_click(on_click_classify) VBox([user_inst,btn_upload,btn_classify,img_user,lbl_pred]) . The final step now is to turn this notebook into a real app. First I create a new notebook containing solely the below code (a combination of the various cells above). . path = Path() learn_inf = load_learner(path/&#39;pokemon.pkl&#39;) user_inst = widgets.Label(&#39;Upload your Pokemon - Gen 1 only please!&#39;) btn_upload = widgets.FileUpload() btn_classify = widgets.Button(description=&#39;Classify&#39;) img_user = widgets.Output() lbl_pred = widgets.Label() def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) img_user.clear_output() with img_user: display(img.to_thumb(128,128)) pred, pred_idx, probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]*100:.02f}%.&#39; btn_classify.on_click(on_click_classify) display(VBox([user_inst,btn_upload,btn_classify,img_user,lbl_pred])) . Next, I view the contents in Voila, which I&#39;ve have already installed on my Deep Learning box. To do this I simply replace /notebooks/ in my url with /voila/render/ and bingo, I can see what my web app will look like. . Finally, I visit Binder to host my new Voila web app. . You can see how I populated the required fields in the image below. Note:- make sure to change the dropdown to the left of the launch button to &#39;URL&#39;. . Additionally, my github username is PDiTO and my repository containing the notebook (in its root folder) is called apps. . . And here is the result with a real life test... . . You can view the app itself here. Enjoy :) . Thanks for spending the time reading! Always happy to answer any questions. .",
            "url": "https://pdito.github.io/blog/deep%20learning/fastai/pokemon/2020/04/03/pokemon-gen1-classifier-blog.html",
            "relUrl": "/deep%20learning/fastai/pokemon/2020/04/03/pokemon-gen1-classifier-blog.html",
            "date": " • Apr 3, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Product Lead Data / Analytics @ FinTech. fast.ai/2020 student. Former: Trader, App Developer. Deep Learning, AI Enthusiast. Unofficial Autism Researcher. . Find me on Twitter and Medium. .",
          "url": "https://pdito.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  
  

  
  

}