{
  
    
        "post0": {
            "title": "MNIST from First Principles",
            "content": "Introduction . Whilst the first 3 chapters of Fastbook cover high level overviews of building a model, productionising a model and ethical considerations of data science, from chapter 4 the book starts to take a more in-depth approach to Deep Learning. . In order to improve my understanding, I decided to rewrite the practical parts of chapters from my own perspective, as I learn. My hope is that these posts will remain succinct in general, closely match the book itself, with room for elaboration for the parts I found most taxing. . I will then tackle a different problem using the techniques learned in an additional blog post, before moving on to the following chapter. . Anyone who finds this useful, spots mistakes, has advice, etc. I would love to hear from you. . Getting Started . We&#39;ll start the usual way, importing the necessary libraries. The call to matplotlib.rc, simply tells the library to default its image colormap to greyscale. We do this as we are working with MNIST data which is a greyscale image set. It will save us specifying that fact every time we want to visualise our data later using imshow(). . from fastai2.vision.all import * from utils import * matplotlib.rc(&#39;image&#39;, cmap=&#39;Greys&#39;) . The focus of this example is to classify any image as either a 3 or 7. We therefore download fastai&#39;s sample of the MNIST data set, which includes numbers from only those categories, represented as 28x28 pixel images. . path = untar_data(URLs.MNIST_SAMPLE) path.ls() . (#3) [Path(&#39;/home/pdito/.fastai/data/mnist_sample/train&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/valid&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/labels.csv&#39;)] . From the above we can see we have a training data folder, train and a validation data folder, valid. Below we see that within these folders we have subfolders for our different classes, 3 and 7. . (path/&#39;train&#39;).ls() . (#2) [Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/7&#39;)] . Finally, let&#39;s take a look inside one of those class folders to understand how the images are named. We sort the ls output here for consistency. . (path/&#39;train/3&#39;).ls().sorted() . (#6131) [Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10000.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10011.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10031.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10034.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10042.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10052.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/1007.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10074.png&#39;),Path(&#39;/home/pdito/.fastai/data/mnist_sample/train/3/10091.png&#39;)...] . Next, let&#39;s create lists to store all the 3s and all the 7s from the training set. . threes = (path/&#39;train/3&#39;).ls().sorted() sevens = (path/&#39;train/7&#39;).ls().sorted() . We can now view the first image from each list, using the Image.open function which is part of the Python Imaging Library, or PIL. . im3 = Image.open(threes[0]) im3 . im7 = Image.open(sevens[0]) im7 . It&#39;s possible to view these numbers as NumPy arrays (or Pytorch tensors) instead, which is how we represent them for the purposes of Machine Learning. . In the below case we take our 3 image and convert it into an array. We then display a slice of the array (which I selected via trial an error to give a good representation) which you can see almost maps out the shape of the 3. . Each number here represents a greyscale number, you can think of 0 being black, 255 being white and anything in between representing the ratio (ie. closer to 0 = darker grey). . array(im3)[5:25,9:21] . array([[ 0, 103, 242, 254, 254, 254, 254, 254, 66, 0, 0, 0], [ 0, 18, 232, 254, 254, 254, 254, 254, 238, 70, 0, 0], [ 0, 0, 104, 244, 254, 224, 254, 254, 254, 141, 0, 0], [ 0, 0, 0, 207, 254, 210, 254, 254, 254, 34, 0, 0], [ 0, 0, 0, 84, 206, 254, 254, 254, 254, 41, 0, 0], [ 0, 0, 0, 0, 24, 209, 254, 254, 254, 171, 0, 0], [ 0, 0, 0, 91, 137, 253, 254, 254, 254, 112, 0, 0], [ 0, 40, 214, 250, 254, 254, 254, 254, 254, 34, 0, 0], [ 0, 81, 247, 254, 254, 254, 254, 254, 254, 146, 0, 0], [ 0, 0, 110, 246, 254, 254, 254, 254, 254, 171, 0, 0], [ 0, 0, 0, 73, 89, 89, 93, 240, 254, 171, 0, 0], [ 0, 0, 0, 0, 0, 0, 1, 128, 254, 219, 31, 0], [ 0, 0, 0, 0, 0, 0, 7, 254, 254, 214, 28, 0], [ 0, 0, 0, 0, 0, 0, 138, 254, 254, 116, 0, 0], [ 0, 0, 0, 0, 0, 25, 240, 254, 254, 34, 0, 0], [ 63, 36, 0, 51, 89, 206, 254, 254, 139, 8, 0, 0], [254, 222, 180, 241, 254, 254, 253, 213, 11, 0, 0, 0], [254, 254, 254, 254, 254, 254, 236, 0, 0, 0, 0, 0], [117, 117, 165, 254, 254, 239, 50, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=uint8) . We can do the same thing for a PyTorch Tensor. . tensor(im3)[5:25,9:21] . tensor([[ 0, 103, 242, 254, 254, 254, 254, 254, 66, 0, 0, 0], [ 0, 18, 232, 254, 254, 254, 254, 254, 238, 70, 0, 0], [ 0, 0, 104, 244, 254, 224, 254, 254, 254, 141, 0, 0], [ 0, 0, 0, 207, 254, 210, 254, 254, 254, 34, 0, 0], [ 0, 0, 0, 84, 206, 254, 254, 254, 254, 41, 0, 0], [ 0, 0, 0, 0, 24, 209, 254, 254, 254, 171, 0, 0], [ 0, 0, 0, 91, 137, 253, 254, 254, 254, 112, 0, 0], [ 0, 40, 214, 250, 254, 254, 254, 254, 254, 34, 0, 0], [ 0, 81, 247, 254, 254, 254, 254, 254, 254, 146, 0, 0], [ 0, 0, 110, 246, 254, 254, 254, 254, 254, 171, 0, 0], [ 0, 0, 0, 73, 89, 89, 93, 240, 254, 171, 0, 0], [ 0, 0, 0, 0, 0, 0, 1, 128, 254, 219, 31, 0], [ 0, 0, 0, 0, 0, 0, 7, 254, 254, 214, 28, 0], [ 0, 0, 0, 0, 0, 0, 138, 254, 254, 116, 0, 0], [ 0, 0, 0, 0, 0, 25, 240, 254, 254, 34, 0, 0], [ 63, 36, 0, 51, 89, 206, 254, 254, 139, 8, 0, 0], [254, 222, 180, 241, 254, 254, 253, 213, 11, 0, 0, 0], [254, 254, 254, 254, 254, 254, 236, 0, 0, 0, 0, 0], [117, 117, 165, 254, 254, 239, 50, 0, 0, 0, 0, 0], [ 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=torch.uint8) . Using Pandas Dataframe, we can even colour code the background to even further illustrate this process. This time without taking a slice, so we can visualise the full image. . df = pd.DataFrame(tensor(im3)) df.style.set_properties(**{&#39;font-size&#39;:&#39;6pt&#39;}).background_gradient(&#39;Greys&#39;) . 0 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 . 0 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 1 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 2 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 3 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 4 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 42 | 118 | 219 | 166 | 118 | 118 | 6 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 5 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 103 | 242 | 254 | 254 | 254 | 254 | 254 | 66 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 6 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 18 | 232 | 254 | 254 | 254 | 254 | 254 | 238 | 70 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 7 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 104 | 244 | 254 | 224 | 254 | 254 | 254 | 141 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 8 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 207 | 254 | 210 | 254 | 254 | 254 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 9 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 84 | 206 | 254 | 254 | 254 | 254 | 41 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 10 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 24 | 209 | 254 | 254 | 254 | 171 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 11 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 91 | 137 | 253 | 254 | 254 | 254 | 112 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 12 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 40 | 214 | 250 | 254 | 254 | 254 | 254 | 254 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 13 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 81 | 247 | 254 | 254 | 254 | 254 | 254 | 254 | 146 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 14 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 110 | 246 | 254 | 254 | 254 | 254 | 254 | 171 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 15 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 73 | 89 | 89 | 93 | 240 | 254 | 171 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 16 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 1 | 128 | 254 | 219 | 31 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 17 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 254 | 254 | 214 | 28 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 18 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 138 | 254 | 254 | 116 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 19 0 | 0 | 0 | 0 | 0 | 0 | 19 | 177 | 90 | 0 | 0 | 0 | 0 | 0 | 25 | 240 | 254 | 254 | 34 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 20 0 | 0 | 0 | 0 | 0 | 0 | 164 | 254 | 215 | 63 | 36 | 0 | 51 | 89 | 206 | 254 | 254 | 139 | 8 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 21 0 | 0 | 0 | 0 | 0 | 0 | 57 | 197 | 254 | 254 | 222 | 180 | 241 | 254 | 254 | 253 | 213 | 11 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 22 0 | 0 | 0 | 0 | 0 | 0 | 0 | 140 | 105 | 254 | 254 | 254 | 254 | 254 | 254 | 236 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 23 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 7 | 117 | 117 | 165 | 254 | 254 | 239 | 50 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 24 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 25 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 26 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . 27 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | . Model 1: Pixel Similarity . For our first attempt at categorisation, we&#39;ll use pixel similarity. In this case we find the average pixel value across all the 3s, and then all the 7s, and we compare our image to that to see which it is most similar too. . To begin with, we need to create tensors to hold our images. First, we&#39;ll create a list of tensors for each image. To make sure it works, we print out the resulting length of each list. . three_tensors = [tensor(Image.open(o)) for o in threes] seven_tensors = [tensor(Image.open(o)) for o in sevens] len(three_tensors),len(seven_tensors) . (6131, 6265) . So far, all looks good. We recall earlier from (path/&#39;train/3&#39;).ls().sorted() that there are 6131 files in the 3 folder. . Let&#39;s view an image to confirm, using fastai&#39;s show_image function (since we now have tensors and not images). . show_image(three_tensors[0]) . &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb0e94fe110&gt; . Next we need to convert our lists of images into rank-3 tensors. In other words, the number of items in the list becomes another dimension of the vector. . Note: rank is the number of axes or dimensions in a tensor; shape is the size of each axis of a tensor. . For our 3s for example, we go from a list of 6131, 28x28 tensors, to a 6131x28x28 rank-3 tensor. PyTorch has a built in function called stack to do this. . Given we need to calculate a mean, we need to cast our integer values to floats, so we do this using the float casting method. We also divide our integers by 255, since in Machine Learning pixel values are expected to be between 0 and 1. . Finally, we call .shape on our tensor to make sure the dimensions are as expected. . stacked_threes = torch.stack(three_tensors).float()/255 stacked_sevens = torch.stack(seven_tensors).float()/255 stacked_threes.shape . torch.Size([6131, 28, 28]) . And to get the rank of the tensor we use: . len(stacked_threes.shape) . 3 . or . stacked_threes.ndim . 3 . To calculate the average across our tensors, we can use the mean function. From the .shape output above we can see the 0 dimension of our tensor represents the number of images, so it is across this axis we wish to calculate the mean. . We can also handily visualise the results to see if what we are doing makes sense. . mean3 = stacked_threes.mean(0) mean7 = stacked_sevens.mean(0) show_image(mean3), show_image(mean7) . (&lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb0e94539d0&gt;, &lt;matplotlib.axes._subplots.AxesSubplot at 0x7fb0e9404dd0&gt;) . Pretty cool! Dark points represent areas where the images generally agree it should be dark, whilst the blurry grey areas represent disagreement between the images. . Now we have our average representations, we can compare our individual image matrix representations to them. . Whilst initially it might seem as simple as taking one away from the other, we have to remember that some differences will result in positive numbers (black areas minus white), whilst others will be negative (white areas minus black) - and these summed would cancel each other out. . Instead, to accurately quantify the difference, we can use the absolute value of the difference (known as the L1 norm) or alternatively the square of the difference square rooted (known as the L2 norm). Both produce a positive error number. . Let&#39;s calculate both for our first 3 that we&#39;ve already spent some time with. . our_3 = stacked_threes[0] dist_3_abs = (our_3 - mean3).abs().mean() dist_3_sqr = ((our_3 - mean3)**2).mean().sqrt() dist_7_abs = (our_3 - mean7).abs().mean() dist_7_sqr = ((our_3 - mean7)**2).mean().sqrt() dist_3_abs, dist_3_sqr, dist_7_abs, dist_7_sqr . (tensor(0.1074), tensor(0.1912), tensor(0.1441), tensor(0.2780)) . In both cases, the distance between our 3 and our ideal 3 is less than the distance between our 3 and our ideal 7. So already, our model appears to give a correct prediction (albeit in a 50/50). Let&#39;s test on our first 7 as well. . our_7 = stacked_sevens[0] dist_3_abs = (our_7 - mean3).abs().mean() dist_3_sqr = ((our_7 - mean3)**2).mean().sqrt() dist_7_abs = (our_7 - mean7).abs().mean() dist_7_sqr = ((our_7 - mean7)**2).mean().sqrt() dist_3_abs, dist_3_sqr, dist_7_abs, dist_7_sqr . (tensor(0.1716), tensor(0.3031), tensor(0.1095), tensor(0.2196)) . In this case the distance between our 7 and our ideal 3 is greater than the distance betwen our 7 and our ideal 7. Another correct prediction. . Note: PyTorch already provides these loss functions in torch.nn.functional which are slightly cleaner to use as illustrated below. We can see the values match. . F.l1_loss(our_7, mean3), F.mse_loss(our_7,mean3).sqrt() . (tensor(0.1716), tensor(0.3031)) . Now our initial sense checked has passed, we want to calculate the actual performance on our validation set. Since error alone is not a good way for humans to understand our performance, we instead measure accuracy. That is, what percentage of our validation images did the model categorise correctly. . To begin with, we must get our validation data sets into tensors in the correct format. . valid_3_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid/3&#39;).ls()]) valid_7_tens = torch.stack([tensor(Image.open(o)) for o in (path/&#39;valid/7&#39;).ls()]) valid_3_tens = valid_3_tens.float()/255 valid_7_tens = valid_7_tens.float()/255 valid_3_tens.shape, valid_7_tens.shape . (torch.Size([1010, 28, 28]), torch.Size([1028, 28, 28])) . As we&#39;re going to be calculating the difference between two &#39;numbers&#39; multiple times, we write a function to handle this. . mean((-1,-2)) represents taking the mean over the last and second last dimensions (in our tensors this is the x and y axis of the image). . def mnist_dist(a, b): return (a-b).abs().mean((-1,-2)) mnist_dist(our_7, mean3) . tensor(0.1716) . So our function works for one example, but what happens if we pass it our tensor of validation images. . valid_3_dist = mnist_dist(valid_3_tens, mean3) valid_3_dist, valid_3_dist.shape . (tensor([0.1745, 0.1193, 0.1334, ..., 0.1181, 0.1370, 0.1406]), torch.Size([1010])) . Instead of throwing an error, PyTorch uses something called broadcasting. This is where PyTorch automatically expands the tensor with the smaller rank to have the same size as the one with the larger rank. . In other words, mean3 moves from being a 28x28 tensor to a 1010x28x28 tensor, where each of the 28x28 images are identical for all 1010 values across the 0 dimension. . This process doesn&#39;t take up additional memory, it is simply implied. Also worth noting, this calc is done in C and on the GPU if you are using CUDA. . Stepping through the mnist_dist function for this calculation:- . a-b - uses broadcasting to subtract mean from every image in our validation set. (We now have a 1010x28x28 tensor). | .abs() - takes the absolute value of every element of our tensor. (We now have a 1010x28x28 tensor). | .mean((-1,-2)), takes the mean across all the values in our last and second to last dimension, in this case our height (28) and width (28) dimensions. (We now have a 1010 tensor). | . Knowing all this allows us to write a function to output our results across an entire validation set. Given we only have two classes, we can write a function called is_3, knowing that if it returns false, this is equivalent to it being a 7. . def is_3(x): return mnist_dist(x , mean3) &lt; mnist_dist(x, mean7) . We can test this on our example cases from earlier, the first 3 and first 7 in our training set. . is_3(three_tensors[0]), is_3(seven_tensors[0]) . (tensor(True), tensor(False)) . It works! is_3 returns True for our first 3 and False for our first 7. Using broadcasting, we are able to perform the same calculation on our entire validation set. . Given a Boolean converted to a float is 0 (False) or 1 (True), we can actually take the mean of our resulting tensors to get the accuracy. . accuracy_3s = is_3(valid_3_tens).float().mean() accuracy_7s = (1 - is_3(valid_7_tens).float()).mean() accuracy_3s, accuracy_7s, (accuracy_3s + accuracy_7s) / 2 . (tensor(0.9168), tensor(0.9854), tensor(0.9511)) . Pretty good results for a really basic model. 91.7% accuracy on 3s, 98.5% accuracy on 7s and 95.1% accuracy overall. . This gives us a good benchmark to see if we can take our performance to the next level. . Model 2: Stochastic Gradient Descent . Our above example doesn&#39;t allow for any improvement in performance, outside of expanding the training set. There is no way for it to get better and better by adjusting parameters. In order to take advantage of deep learning, we need to come up with a function which has weights that can be updated. . A sensible approach in this case would be a function that represents each pixel as having a weight, with that weight being higher for those pixels most likely to be white (remember in MNIST our background is black and writing is white) for a particularly category. . For example: . def pr_three(x, w) = (x * w).sum() . In the above example, X represents our image, in this case a vector. Previously we&#39;ve regarded our image as a 28x28 tensor, but if we stack up all the rows in a single line, we end up with a 784 vector (which can still be called a tensor). . Likewise, W represents our weights. We can gradually update these weights, making them a little bit better each time, until they are able to best predict if a given image is a 3 or not. . The process is as follows:- . Initialise the weights. | For each image use these weights to predict whether the image is a 3 or 7. | Based on these predictions, calculate the model&#39;s performance (its loss). | Calculate the gradient, which measures for each weight, how changing that weight would change the loss. | Step (that is, change) all weights based on that calculation. | Go back to step 2, and repeat. | Stop - when you decide to (because the model is good enough or you don&#39;t want to wait any longer, or performance stops improving). | gv(&#39;&#39;&#39; init-&gt;predict-&gt;loss-&gt;gradient-&gt;step-&gt;stop step-&gt;predict[label=repeat] &#39;&#39;&#39;) . &lt;!DOCTYPE svg PUBLIC &quot;-//W3C//DTD SVG 1.1//EN&quot; &quot;http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd&quot;&gt; G init init predict predict init&#45;&gt;predict loss loss predict&#45;&gt;loss gradient gradient loss&#45;&gt;gradient step step gradient&#45;&gt;step step&#45;&gt;predict repeat stop stop step&#45;&gt;stop Let&#39;s get back to our MNIST sample dataset of 3s and 7s. First we combine our 3s and 7s into the same tensor. . train_x = torch.cat([stacked_threes, stacked_sevens]) train_x.shape . torch.Size([12396, 28, 28]) . Next, as explained earlier, we want to flatten our image into a single vector, resulting in a tensor of the form (images, pixel values) where images represents every image and pixel values represents the pixel values in vector form of any given image. . To do this we can use PyTorch&#39;s view method, which changes the shape of a tensor without changing its contents. The -1 is a special parameter that says, &#39;make this axis as big as necessary to fit all the data&#39; rather than setting it to a specific value. . train_x = train_x.view(-1, 28*28) train_x.shape . torch.Size([12396, 784]) . Now we have our image data, but since we have combined 3s and 7s into the same dataset, we need to label those values so our model can calculate the loss. We&#39;ll use a 1 for threes and 0 for 7s. . Because we know we have combined our two datasets, we can simply take the length of threes and make the y values 1s, the length of sevens and make them 0s, and then combine. These will match up with our train_x tensor. . train_y = tensor([1]*len(threes) + [0]*len(sevens)) train_y.shape . torch.Size([12396]) . For convention, we need to convert this vector into a matrix, in this case of 12396 rows and 1 column. To do this we can use the unsqueeze method, and specify the position 1 dimension (remember these are zero base). . train_y = train_y.unsqueeze(1) train_y.shape . torch.Size([12396, 1]) . Now we have our data structured in the way we want, we need to combine it into a Dataset that PyTorch can use. It expects to be able to index in to a DataSet and get a tuple of (x, y) returned. We can combine the zip and list Python functions to achieve this. . dset = list(zip(train_x, train_y)) x, y = dset[0] x.shape, y.shape . (torch.Size([784]), torch.Size([1])) . Next we do the same for our validation data. . valid_x = torch.cat([valid_3_tens, valid_7_tens]).view(-1, 28*28) valid_y = tensor([1] * len(valid_3_tens) + [0] * len(valid_7_tens)).unsqueeze(1) valid_dset = list(zip(valid_x, valid_y)) x, y = valid_dset[0] x.shape, y.shape . (torch.Size([784]), torch.Size([1])) . Now we need to create our weights tensor. The best way to do this is with an initialisation function, since we want our initial parameters to be random and we don&#39;t want to have to repeat that process every time we use this model. . We also add the requiresgrad() method to tell PyTorch we want to track the gradients of the parameters in this tensor. We&#39;ll touch on this in more detail later. . def init_params(size, std=1.0): return (torch.randn(size)*std).requires_grad_() weights = init_params((28*28, 1)) . A problem with our proposed function of x * w is that where x (ie the pixel value) is zero, then x * w will be zero. The gradient will be zero, and the model won&#39;t be able to improve. To overcome this problem, we add a bias unit. (Remember from school, the straight line equation of y = wx + b. . bias = init_params(1) . Note, in neural networks, w = weights, b = bias and together these make up the parameters. . Let&#39;s go ahead and calculate the loss for one image. Note, .T simply transposes the weights, which is necessary to perform the multiplication. . (train_x[0]*weights.T).sum()+bias . tensor([10.2040], grad_fn=&lt;AddBackward0&gt;) . Clearly we have the option here to loop through every image and sum the results to calculate our overall loss, but this would be very slow. Instead, we use matrix multiplication (view here if unsure how it works), which calculates w * x for every row of the matrix. This is the fundamental mathematical operation that powers all of deep learning. . In Python, matrix multiplication is represented by the @ operator. We create the function below to perform this for an entire DataSet. Note:- xb refers to x-batch, ie. a mini batch of our training data. No need to worry about what that is for now. . def linear1(xb): return xb@weights + bias preds = linear1(train_x) preds . tensor([[10.2040], [ 1.3076], [ 6.9170], ..., [ 8.5185], [ 4.9300], [ 0.4510]], grad_fn=&lt;AddBackward0&gt;) . To check our accuracy, lets see if our preds results are greater than zero. This confused me at first, but if you look how we initialise our parameters, we use a normal distribution with a standard deviation of 1. So it seems likely that around half of our predictions will be positive and half will be negative. Therefore we pick 0.0 for training efficiency purposes. . It&#39;s worth noting that we could have picked any number for the threshold, but picking 0.0 saves our model from having to &#39;drag&#39; its values away from where they were initialised to having a mean around a different threshold. The end result and accuracy would be same, it may (although not necessarily) just take a little longer to get there. . corrects = (preds&gt;0.0).float() == train_y corrects . tensor([[ True], [ True], [ True], ..., [False], [False], [False]]) . corrects.float().mean().item() . 0.4757179617881775 . Given we are using a normal distribution with a standard deviation of 1.0 to initialise our weights, it makes sense our model is around 50% accurate (since it has a 50/50 chance of being so). . So what happens when we adjust one of our weights: . weights[0] *= 1.0001 preds = linear1(train_x) ((preds &gt; 0.0).float() == train_y).float().mean().item() . 0.4757179617881775 . It looks like our weight&#39;s gradient is 0 with respect to accuracy. There is a simple explanation for this. Changing the weight for 1 pixel in the image by a tiny amount, will, in most cases, fail to change our model&#39;s final prediction. Accuracy is a direct function of this prediction. . So instead, we need a measure of performance that will respond better to minor changes in pixel weightings. We call this a loss function. . Since we know our y values are either 1 or 0, we can simply measure the error as the difference between expected result and prediction. In other words, if our prediction was 0.8, and the y value was 1 (it was 3!) then our loss would be 0.2. If our prediction was 0.8 and the y value was 0 (it was a 7) then our loss would be 0.8. . For this to work, we need to ensure our predictions always lie between 0 and 1. To do that, we can use the sigmoid function. . plot_function(torch.sigmoid, min=-5, max=5) . So let&#39;s create our loss function. We use torch.where which is basically a far more efficient CUDA speed list comprehension. In this case our function says if the prediction should be 1, then calculate the loss as 1 - prediction, otherwise set the loss to our prediction, given the only other value is 0 (and so loss would be prediction - 0). . Before passing our values into this function, we run .sigmoid() to make sure all our predictions fall between 1 and 0. . def mnist_loss(predictions, targets): predictions = predictions.sigmoid() return torch.where(targets==1, 1 - predictions, predictions).mean() . In simple terms, once we introduce this loss function, our model should learn that a higher prediction corressponds to more confidence an image is a 3. . Now we have our data and loss function, we need to train our model. Instead of doing this image by image and calculating the gradients each time, a better approach is to use a mini-batch of images, calculating the average loss for those as a group, and the subsequent gradients. This allows our model to learn through an epoch, not only at the end of an entire epoch. . To pass these batches we use a PyTorch class called a DataLoader. Remember, we already created our Dataset, dset and Validation Dataset, valid_dset earlier. We can pass these into DataLoader with a desired batch_size to create our DataLoaders. . dl = DataLoader(dset, batch_size=256) valid_dl = DataLoader(valid_dset, batch_size=256) . Next let&#39;s create a function to calculate our gradients at each step. We pass the function a mini-batch of training data (xb) and labels (yb) along with our model, calculate our predictions, calculate our subsequent loss and then .backward() calculates the gradients on our parameters with respect to that loss function. . The idea is that whenever we call .backward() on a function, it computes the gradient with respect to that function for every parameter which has requires_grad=True. The key word here is EVERY. At first I was wondering how we linked our loss function to our parameters. The answer is, we don&#39;t need to, because its done for everything automatically. . def calc_grad(xb, yb, model): preds = model(xb) loss = mnist_loss(preds, yb) loss.backward() . Now we&#39;re in a position to create our basic training loop. Training a model often takes many epochs. An epoch is simply one full pass through the model of all your training data. Too many epochs and you&#39;re likely to overfit your training data, so its important to keep an eye on the direction of your validation metric. . Here the function loops through every mini-batch in our training set, calculating the gradients on our parameters at each step and then adjusting the parameters by those gradients multiplied by a pre-specified learning rate. . Note: once we&#39;ve adjusted our parameters, we zero out our gradients since .backward() adds the gradients to any that are already stored, which is not what we want to do across mini-batches. . def train_epoch(model, lr, params): for xb, yb in dl: calc_grad(xb, yb, model) for p in params: p.data -= p.grad * lr p.grad.zero_() . Finally we need functions to calculate our accuracy, and perform this calculation on our validation set after each epoch. . Our batch_accuracy function takes a batch from our validation data and returns its accuracy, whilst validate_epoch uses this function when looping through ALL our valid_dl Dataset to calculate an average of accuracy over all our mini-batches. . def batch_accuracy(xb, yb): preds = xb.sigmoid() correct = (preds &gt; 0.5) == yb return correct.float().mean() def validate_epoch(model): accs = [batch_accuracy(model(xb), yb) for xb, yb in valid_dl] return round(torch.stack(accs).mean().item(), 6) . Now lets re-initialise our parameters. . weights = init_params((28*28,1)) bias = init_params(1) params = weights, bias . And set our model into action... . epochs_to_train = 20 lr = 1. for i in range(epochs_to_train): train_epoch(linear1, lr, params) print(validate_epoch(linear1), end=&#39; &#39;) . 0.774255 0.898747 0.93486 0.951462 0.956344 0.960251 0.961227 0.9632 0.963689 0.965153 0.965642 0.966618 0.967595 0.968083 0.96906 0.968083 0.968571 0.969568 0.969568 0.969568 . Good progress, 97% accurancy. we&#39;re beating our original model after training for just 20 epochs, which took no time at all. . Model 3: Stochastic Gradient Descent (Again) - with PyTorch. . The above breakdown was really there to aid understanding, the code can be simplified considerably using PyTorch&#39;s built in functionality. . The nn.Linear function replaces both our init_params ands linear functions above. . linear_model = nn.Linear(28*28, 1) . Our weights and bias are contained within a single class as shown below:- . w, b = linear_model.parameters() w.shape, b.shape . (torch.Size([1, 784]), torch.Size([1])) . We can create then an Optimiser class to use this model to train. . class BasicOptim: def __init__(self, params, lr): self.params, self.lr = list(params), lr def step(self, *args, **kwargs): for p in self.params: p.data -= p.grad.data * self.lr def zero_grad(self, *args, **kwargs): for p in self.params: p.grad = None . We initialise our Optimiser as follows. . opt = BasicOptim(linear_model.parameters(), lr) . Then we can once again create our training function. . def train_epoch(model): for xb, yb in dl: calc_grad(xb, yb, model) opt.step() opt.zero_grad() . And our training loop. . def train_model(model, epochs): for i in range(epochs): train_epoch(model) print(validate_epoch(model), end=&#39; &#39;) . And we run 20 epochs for this model... . train_model(linear_model, 20) . 0.4932 0.7627 0.8554 0.9155 0.9345 0.9477 0.956 0.9633 0.9658 0.9672 0.9697 0.9716 0.9736 0.9746 0.9755 0.9765 0.9775 0.9775 0.978 0.9785 . We get very similar results here again, 98% accuracy. . &#160;Model 4 - Stochastic Gradient Descent (Again, Again) - with fastai v2 . fastai provides its own SGD class which does the same as our BasicOptim class above. . This allows for further simplification. . linear_model = nn.Linear(28*28, 1) opt = SGD(linear_model.parameters(), lr) train_model(linear_model, 20) . 0.493164 0.765605 0.856894 0.918417 0.935995 0.95162 0.956503 0.962363 0.965781 0.967734 0.970663 0.972617 0.97457 0.975058 0.976035 0.976523 0.977499 0.977988 0.978476 0.978476 . Again, the same result. . Model 5 - Stocashtic Gradient Descent - fastai v2 Learner . We can again reduce the code required by using fastai&#39;s Learner class. This reduces the need for our training functions / loop. We must first create DataLoaders which pass fastai batches of our data. We then create our Learner by passing in our Dataloaders, the model, the optimisation function, the loss function and (optionally) our metrics. . dls = DataLoaders(dl, valid_dl) learn = Learner(dls, nn.Linear(28*28,1), opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) learn.fit(20, lr=lr) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.636961 | 0.503404 | 0.495584 | 00:00 | . 1 | 0.494130 | 0.196144 | 0.835132 | 00:00 | . 2 | 0.182421 | 0.174930 | 0.843474 | 00:00 | . 3 | 0.080600 | 0.104251 | 0.912169 | 00:00 | . 4 | 0.043039 | 0.076627 | 0.934740 | 00:00 | . 5 | 0.028333 | 0.061650 | 0.947988 | 00:00 | . 6 | 0.022267 | 0.052274 | 0.956330 | 00:00 | . 7 | 0.019558 | 0.045992 | 0.963199 | 00:00 | . 8 | 0.018177 | 0.041555 | 0.965653 | 00:00 | . 9 | 0.017346 | 0.038275 | 0.967125 | 00:00 | . 10 | 0.016761 | 0.035755 | 0.970069 | 00:00 | . 11 | 0.016301 | 0.033752 | 0.972031 | 00:00 | . 12 | 0.015918 | 0.032116 | 0.973503 | 00:00 | . 13 | 0.015591 | 0.030750 | 0.974485 | 00:00 | . 14 | 0.015310 | 0.029594 | 0.975957 | 00:00 | . 15 | 0.015066 | 0.028604 | 0.976448 | 00:00 | . 16 | 0.014853 | 0.027748 | 0.977429 | 00:00 | . 17 | 0.014664 | 0.027003 | 0.977920 | 00:00 | . 18 | 0.014495 | 0.026348 | 0.978410 | 00:00 | . 19 | 0.014341 | 0.025770 | 0.978410 | 00:00 | . Again, less code, same results, around 98% accuracy. . &#160;Model 6 - Stochastic Gradient Descent - Adding a Non-linearity . So far, all our models have used a simple linear classifier. To add complexity to our model, enabling it to perform more detailed tasks, we need can add a non-linearity between two linear classifiers (since two linear classifiers on their own can be simplified into a single linear classifier). It is this basic concept that gives us the foundation of a very simple neural network. . This simple neural network can be summarised as:- . neural_net = xb@w1 + b1 neural_net = neural_net.max(tensor(0.0)) neural_net = neural_net@w2 + b2 . That&#39;s really all there is to it. Lines 1 and 3 represent linear classifiers which we are already familiar with, line 2 represents the ReLU function - a complicated way of saying take the maximum of the value or 0. . So how do we code this model. It can be done in 1 line in PyTorch. nn.Sequential creates a module which calls each of the listed layers in turn. . Note: The 30s in the below code represent the number of output activations for w1 and input activations for w2. These are in effect our features. So the model can construct 30 different features, each representing a different mix of pixels, and pass them as inputs (after running through the ReLU function) to the following layer. The higher this number, the higher the complexity of the model. . neural_net = nn.Sequential( nn.Linear(28*28, 30), nn.ReLU(), nn.Linear(30,1) ) . Let&#39;s see if this helps improve our accuracy. We do train for more epochs here, but use a much lower learning rate. . learn = Learner(dls, neural_net, opt_func=SGD, loss_func=mnist_loss, metrics=batch_accuracy) learn.fit(40, 0.1) . epoch train_loss valid_loss batch_accuracy time . 0 | 0.349672 | 0.389846 | 0.513248 | 00:00 | . 1 | 0.157197 | 0.239514 | 0.794897 | 00:00 | . 2 | 0.085345 | 0.117664 | 0.914132 | 00:00 | . 3 | 0.054955 | 0.078449 | 0.941119 | 00:00 | . 4 | 0.041037 | 0.060892 | 0.955348 | 00:00 | . 5 | 0.034031 | 0.051127 | 0.963690 | 00:00 | . 6 | 0.030067 | 0.044998 | 0.967125 | 00:00 | . 7 | 0.027523 | 0.040819 | 0.967125 | 00:00 | . 8 | 0.025706 | 0.037790 | 0.968597 | 00:00 | . 9 | 0.024309 | 0.035483 | 0.971050 | 00:00 | . 10 | 0.023182 | 0.033657 | 0.972031 | 00:00 | . 11 | 0.022244 | 0.032163 | 0.973013 | 00:00 | . 12 | 0.021448 | 0.030910 | 0.974975 | 00:00 | . 13 | 0.020758 | 0.029838 | 0.974975 | 00:00 | . 14 | 0.020154 | 0.028907 | 0.974975 | 00:00 | . 15 | 0.019618 | 0.028087 | 0.975957 | 00:00 | . 16 | 0.019139 | 0.027358 | 0.976938 | 00:00 | . 17 | 0.018706 | 0.026706 | 0.977429 | 00:00 | . 18 | 0.018313 | 0.026118 | 0.978410 | 00:00 | . 19 | 0.017954 | 0.025585 | 0.978410 | 00:00 | . 20 | 0.017624 | 0.025099 | 0.978901 | 00:00 | . 21 | 0.017318 | 0.024655 | 0.978901 | 00:00 | . 22 | 0.017034 | 0.024248 | 0.979882 | 00:00 | . 23 | 0.016769 | 0.023873 | 0.979882 | 00:00 | . 24 | 0.016521 | 0.023527 | 0.980373 | 00:00 | . 25 | 0.016287 | 0.023207 | 0.982336 | 00:00 | . 26 | 0.016067 | 0.022910 | 0.982336 | 00:00 | . 27 | 0.015860 | 0.022634 | 0.982336 | 00:00 | . 28 | 0.015663 | 0.022377 | 0.982336 | 00:00 | . 29 | 0.015476 | 0.022137 | 0.982336 | 00:00 | . 30 | 0.015298 | 0.021913 | 0.982336 | 00:00 | . 31 | 0.015129 | 0.021703 | 0.982826 | 00:00 | . 32 | 0.014968 | 0.021506 | 0.982826 | 00:00 | . 33 | 0.014813 | 0.021322 | 0.982826 | 00:00 | . 34 | 0.014665 | 0.021148 | 0.982826 | 00:00 | . 35 | 0.014523 | 0.020984 | 0.982826 | 00:00 | . 36 | 0.014387 | 0.020829 | 0.982826 | 00:00 | . 37 | 0.014256 | 0.020682 | 0.982826 | 00:00 | . 38 | 0.014129 | 0.020543 | 0.982826 | 00:00 | . 39 | 0.014007 | 0.020411 | 0.983317 | 00:00 | . Our accuracy has increased to above 98%! . Model 7 - Stochastic Gradient Descent - ResNet18 . We needn&#39;t stop there. Whilst simplicity is typically preferred, its been proven that networks with a higher number of smaller layers get better than results that those with a lower number of larger layers. This also has an efficiency benefit, with these models being faster to train. . As a final approach, let&#39;s train a model with 18 layers. We are using an architecture called resnet18, and a cross entropy loss function, but we won&#39;t go into detail on those here. The purpose is to see the additional accuracy that can be achieved using these deeper networks. . dls = ImageDataLoaders.from_folder(path) learn = cnn_learner(dls, resnet18, pretrained=False, loss_func=F.cross_entropy, metrics=accuracy) learn.fit_one_cycle(1, 0.1) . epoch train_loss valid_loss accuracy time . 0 | 0.088288 | 0.008928 | 0.997547 | 00:06 | . 99.75% accuracy after one epoch. Not bad! . Summary . We&#39;ve seen many different applications of the same solution, whilst also upping the complexity towards the end to understand the performance improvements. . The next step is to take what we have learned and apply it to the full MNIST training set, for digits 0 thru 9. .",
            "url": "https://pdito.github.io/blog/deep%20learning/fastai/gradient%20descent/sgd/mnist/2020/04/17/fastai2-ch4-mnist-first-principles.html",
            "relUrl": "/deep%20learning/fastai/gradient%20descent/sgd/mnist/2020/04/17/fastai2-ch4-mnist-first-principles.html",
            "date": " â€¢ Apr 17, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "A Pokemon Gen 1 Classifier",
            "content": "The Project . Following completion of week 2 of the fastai 2020 course, here is my first project, a Pokemon Gen 1 Classifier. My target is to achieve 90% accuracy metric. As I write, I have no idea how ambitious that is. . The Data . One thing i&#39;ve learned very quickly about Deep Learning, is that gathering and labelling data is often the biggest challenge. Thankfully, i&#39;ve also learned that the community is incredibly generous in sharing data. I definitely intend to gather my own data soon - I feel that&#39;s something key to get experience in early on, but for my first project I wanted to dive into the coding as soon as possible. . I found an excellent dataset on Kaggle perfectly suited to my problem. So big thanks to Lance Zhang for this. . Steps . I&#39;m trying to keep this model as simple as possible, and hope the below steps will cover everything. . Set up. | Get data and build data model. | Train model. | Test on new data. | Productionise. | 1. Set Up . I&#39;m fortunate to have a local Ubuntu box with a GPU capable of training these kind of models (I&#39;ll post about setting that up soon). But typically it is recommended to use something like Paperspace or Colab when starting out and the below should be applicable to any environment you intend to run Jupyter on. . First step, import the necessary libraries. . from utils import * from fastai2.vision.widgets import * . TIP: This initially failed for me. I had to ensure that utils.py, which sits in the fast.ai nbs (notebooks) directory, was copied to the same folder as the notebook I was working on. . 2. The Data Model . There are many ways to get data into your deep learning environment. Kaggle offers a library, which I&#39;ll attempt to use in future, but for now, what I found most comfortable was to download the data to my local machine, inspect it, get to know it, zip it up and put that zip file on Dropbox. I was then able to share that file, and use wget {link} to import it to my deep learning environment. . One point worth noting, is that when you share a Dropbox link, the end part always defaults to dl=0, you should change this to dl=1 for the wget command to work. I believe this option is only available to paid Dropbox accounts, so please bear that in mind too. . You could also upload the .zip via Jupyter&#39;s upload function. . After this step I manually unzipped the file. fast.ai has some nice convenience methods to download and untar data, but for now I don&#39;t believe the library has native support for zipped data. . Below I set the path for my data. . path = Path(&quot;/home/pdito/AI/datasets/pokemon-gen1/&quot;) path.ls() . (#150) [Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Psyduck&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Bulbasaur&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Poliwhirl&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Raticate&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Victreebel&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Tangela&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Venusaur&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Butterfree&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Cubone&#39;),Path(&#39;/home/pdito/AI/datasets/pokemon-gen1/Gastly&#39;)...] . We can see the data is split into folders for each different Pokemon, but there is no training / validation split currently. . Next I create my Datablock. . I have two blocks, and ImageBlock and a CategoryBlock. The ImageBlock represents by independent variables, whilst the CategoryBlock represents the dependent value, in other words, my y or what i&#39;m trying to predict. . In order to get the items, I set this to get_image_files - which recursively polls a provided path to extract all the images. I&#39;m actually still not sure why we don&#39;t specify the path in the DataBlock, and instead call it later when we initialise the DataLoader. I guess the DataBlock is simply defining the structure and the DataLoader is actually initialising something with real data. . Splitter determines my training / validation set split, i&#39;ve chosen to make this 80% / 20% and set a seed so the validation set remains consistent as I iterate. . I set my Category / dependent variable / y to parent_label, as my images are all in subfolders with the Pokemon name. This is super useful part of the fastai library, and I understand there are lots of different options available. . Finally, I transform all my images using a standard Resize. This makes the images 224x224, which should work well as i&#39;m intending to use transfer learning from ResNet34, which was trained on images of this dimension. . pokemon = DataBlock( blocks=(ImageBlock, CategoryBlock), get_items=get_image_files, splitter=RandomSplitter(valid_pct=0.2, seed=42), get_y=parent_label, item_tfms=Resize(224)) . Next, from my DataBlock, I create my DataLoaders (train and valid) referencing the root folder of my images that we already specified above. . dls = pokemon.dataloaders(path) . I can now visualise the DataLoaders to make sure everything looks correct. . dls.valid.show_batch(max_n=4, nrows=1) . All looks good, so its time to define our architecture. In this case, i&#39;m using the pre-trained resnet34 architecture as a starting point. I&#39;ll then run fastai&#39;s fine tune function (which is designed specifically for transfer learning) to repurpose the model to my requirements (by adding some additional layers to the head and retraining parts of the models with differing learning rates - we don&#39;t need to get into those details at this stage). . I set the metric to accuracy, which I think is the best, human-intepretable output at this stage. . learn = cnn_learner(dls, resnet34, metrics=accuracy) learn.fine_tune(4) . epoch train_loss valid_loss accuracy time . 0 | 4.545189 | 1.752553 | 0.603968 | 00:13 | . epoch train_loss valid_loss accuracy time . 0 | 1.387421 | 0.610193 | 0.854519 | 00:18 | . 1 | 0.632813 | 0.358847 | 0.911830 | 00:18 | . 2 | 0.244674 | 0.301103 | 0.929464 | 00:18 | . 3 | 0.120268 | 0.288353 | 0.930198 | 00:18 | . Surprisingly, with almost no effort, I&#39;m able to achieve over 93% accuracy on the validation set. This is particularly impressive given the structure of the input data. Images include the official animation images, pixelated gameboy images, plush toy images, hand-drawn images. Furthermore, many of the Pokemon in gen 1 have an extremely close likeness amongst their evolution tree. . I had expected to have to fine tune the model and tidy up the input data to get anywhere near this result. Instead, in less than a minute and half of training, i&#39;ve exceeded my initial target. . This is a great indication of the power of transfer learning. I&#39;m keen to revisit this model in future to see how I can improve the accuracy. . Even though I am happy with the results, it&#39;s interesting to look at the items that the model is most incorrect on (in other words, from the set classifications it got incorrect, those which it had most confidence it got correct). . Often these results are visualised in a confusion matrix, but we have too many classes here (151) so instead we just straight to the top losses. . interp = ClassificationInterpretation.from_learner(learn) . #interp.plot_top_losses(5, nrows=1) interp.plot_top_losses(6, figsize=(15,10)) . Seeing this visualisation helps me understand why the model is making mistakes. . Top Left: This image should be excluded. It shows multiple Pokemon. Top Middle: So close! It was obviously hard for the model to pick up detail in this dimly lit image. Top Right: Mislabelled data! Our model is actually correct. This is Alakazam and not Kadabra. Bottom Left: I feel like our model should have got this one, but admitedly the angle is strange. Bottm Middle: It&#39;s unusual to see Marowak with fire, and the colour palette is far more attuned to Magmar. Bottom Right: You can certainly see why the model would choose Gyarados. At a glance, I would do the same. . Overall, there is nothing here that gives me particular concern. . 4. Quick Test . I use a widget to easily allow me to upload files for testing from my local machine. Here I run a test against an image of Pikachu that is not included in the original data set. . uploader = widgets.FileUpload() uploader . img = PILImage.create(uploader.data[0]) category,pred_idx,probs = learn.predict(img) print(f&quot;Who is this?: {category}. Probability: {probs[pred_idx] * 100:.02f}%.&quot;) img . Who is this?: Pikachu. Probability: 99.96%. . It works! . 5. Productionise . If getting good, labelled data is the hardest part of the problem, i&#39;m told productionising your model comes a close second. . I&#39;m going to build this out into a separate notebook and use Voila (to turn a notebook into an app) and Binder (to publish it on the web). Nethertheless, for completeness I&#39;ll provide all the detail below. . First, let&#39;s export our trained model and confirm that the file exists after doing so. . learn.export() path = Path() path.ls(file_exts=&#39;.pkl&#39;) . (#1) [Path(&#39;export.pkl&#39;)] . I&#39;m renaming the file in terminal from export.pkl to pokemon.pkl so I understand what it is...going forward it will be refered to as the latter. . Everything below this point will form the basis of our &#39;Web App&#39;. . In Deep Learning, prediction is often refered to as inference. I&#39;m going to load in my trained model for inference below. . learn_inf = load_learner(path/&#39;pokemon.pkl&#39;) . We can check that all seems correct by reviewing our potential output categories. . learn_inf.dls.vocab . (#150) [&#39;Abra&#39;,&#39;Aerodactyl&#39;,&#39;Alakazam&#39;,&#39;Alolan Sandslash&#39;,&#39;Arbok&#39;,&#39;Arcanine&#39;,&#39;Articuno&#39;,&#39;Beedrill&#39;,&#39;Bellsprout&#39;,&#39;Blastoise&#39;...] . Seems good! . First we create a label to guide the user. Next we create an Upload button so our user can submit their image. We then create a Classify button so the user can run the classification. Finally we create placeholders to display the uploaded image and the resulting classification. . After that we add an event handler to the Classify button to perform the necessary actions and populate the output placeholders on button click. . Finally, we place all the UI elements into a vertical box (VBox) to keep the UI looking clean. . user_inst = widgets.Label(&#39;Upload your Pokemon - Gen 1 only please!&#39;) btn_upload = widgets.FileUpload() btn_classify = widgets.Button(description=&#39;Classify&#39;) img_user = widgets.Output() lbl_pred = widgets.Label() def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) img_user.clear_output() with img_user: display(img.to_thumb(128,128)) pred, pred_idx, probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]*100:.02f}%.&#39; btn_classify.on_click(on_click_classify) VBox([user_inst,btn_upload,btn_classify,img_user,lbl_pred]) . The final step now is to turn this notebook into a real app. First I create a new notebook containing solely the below code (a combination of the various cells above). . path = Path() learn_inf = load_learner(path/&#39;pokemon.pkl&#39;) user_inst = widgets.Label(&#39;Upload your Pokemon - Gen 1 only please!&#39;) btn_upload = widgets.FileUpload() btn_classify = widgets.Button(description=&#39;Classify&#39;) img_user = widgets.Output() lbl_pred = widgets.Label() def on_click_classify(change): img = PILImage.create(btn_upload.data[-1]) img_user.clear_output() with img_user: display(img.to_thumb(128,128)) pred, pred_idx, probs = learn_inf.predict(img) lbl_pred.value = f&#39;Prediction: {pred}; Probability: {probs[pred_idx]*100:.02f}%.&#39; btn_classify.on_click(on_click_classify) display(VBox([user_inst,btn_upload,btn_classify,img_user,lbl_pred])) . Next, I view the contents in Voila, which I&#39;ve have already installed on my Deep Learning box. To do this I simply replace /notebooks/ in my url with /voila/render/ and bingo, I can see what my web app will look like. . Finally, I visit Binder to host my new Voila web app. . You can see how I populated the required fields in the image below. Note:- make sure to change the dropdown to the left of the launch button to &#39;URL&#39;. . Additionally, my github username is PDiTO and my repository containing the notebook (in its root folder) is called apps. . . And here is the result with a real life test... . . You can view the app itself here. Enjoy :) . Thanks for spending the time reading! Always happy to answer any questions. .",
            "url": "https://pdito.github.io/blog/deep%20learning/fastai/pokemon/2020/04/03/pokemon-gen1-classifier-blog.html",
            "relUrl": "/deep%20learning/fastai/pokemon/2020/04/03/pokemon-gen1-classifier-blog.html",
            "date": " â€¢ Apr 3, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Me",
          "content": "Product Lead Data / Analytics @ FinTech. fast.ai/2020 student. Former: Trader, App Developer. Deep Learning, AI Enthusiast. Unofficial Autism Researcher. . Find me on Twitter and Medium. .",
          "url": "https://pdito.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  
  

  
  

}